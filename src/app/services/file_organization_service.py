"""
íŒŒì¼ ì •ë¦¬ ì„œë¹„ìŠ¤

MainWindowì˜ íŒŒì¼ ì •ë¦¬ ë¡œì§ì„ ë¶„ë¦¬í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ í†µí•´ UIë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•Šê³  íŒŒì¼ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import logging
import shutil
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any
from uuid import UUID, uuid4

from src.core.file_parser import FileParser

from src.app.background_task import BaseTask, TaskResult, TaskStatus
from src.app.events import TypedEventBus
from src.app.organization_events import (OrganizationCancelledEvent,
                                   OrganizationCompletedEvent,
                                   OrganizationErrorType,
                                   OrganizationFailedEvent,
                                   OrganizationPreflightCompletedEvent,
                                   OrganizationPreflightData,
                                   OrganizationPreflightStartedEvent,
                                   OrganizationProgressEvent,
                                   OrganizationResult,
                                   OrganizationStartedEvent,
                                   OrganizationStatus,
                                   OrganizationValidationCompletedEvent,
                                   OrganizationValidationFailedEvent,
                                   OrganizationValidationResult,
                                   OrganizationValidationStartedEvent)
from src.app.services.background_task_service import IBackgroundTaskService


class IFileOrganizationService(ABC):
    """íŒŒì¼ ì •ë¦¬ ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def validate_organization_request(
        self, grouped_items: dict[str, Any], destination_directory: Path
    ) -> OrganizationValidationResult:
        """íŒŒì¼ ì •ë¦¬ ìš”ì²­ ê²€ì¦"""

    @abstractmethod
    def create_preflight_data(
        self, grouped_items: dict[str, Any], destination_directory: Path
    ) -> OrganizationPreflightData:
        """í”„ë¦¬í”Œë¼ì´íŠ¸ ë°ì´í„° ìƒì„±"""

    @abstractmethod
    def start_organization(
        self, grouped_items: dict[str, Any], destination_directory: Path, dry_run: bool = False
    ) -> UUID:
        """íŒŒì¼ ì •ë¦¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰, ì¡°ì§í™” ID ë°˜í™˜)"""

    @abstractmethod
    def cancel_organization(self, organization_id: UUID) -> bool:
        """íŒŒì¼ ì •ë¦¬ ì·¨ì†Œ"""

    @abstractmethod
    def get_organization_status(self, organization_id: UUID) -> OrganizationStatus | None:
        """íŒŒì¼ ì •ë¦¬ ìƒíƒœ ì¡°íšŒ"""

    @abstractmethod
    def dispose(self) -> None:
        """ì„œë¹„ìŠ¤ ì •ë¦¬"""


class FileOrganizationTask(BaseTask):
    """íŒŒì¼ ì •ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""

    def __init__(
        self,
        organization_id: UUID,
        grouped_items: dict[str, Any],
        destination_directory: Path,
        event_bus: TypedEventBus,
        dry_run: bool = False,
    ):
        super().__init__(event_bus, f"íŒŒì¼ ì •ë¦¬: {destination_directory.name}")
        self.organization_id = organization_id
        self.grouped_items = grouped_items
        self.destination_directory = destination_directory
        self.event_bus = event_bus
        self.dry_run = dry_run
        self.logger = logging.getLogger(f"{self.__class__.__name__}_{organization_id}")
        self._cancelled = False
        self.file_parser = FileParser()

    def execute(self) -> TaskResult:
        """íŒŒì¼ ì •ë¦¬ ì‹¤í–‰"""
        try:
            self.logger.info(f"íŒŒì¼ ì •ë¦¬ ì‘ì—… ì‹œì‘: {self.destination_directory}")
            start_time = time.time()

            # ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
            result = OrganizationResult()

            # ì‹œì‘ ì´ë²¤íŠ¸ ë°œí–‰
            total_files = self._count_total_files(self.grouped_items)
            total_groups = len(self.grouped_items)

            self.event_bus.publish(
                OrganizationStartedEvent(
                    organization_id=self.organization_id,
                    destination_directory=self.destination_directory,
                    total_files=total_files,
                    total_groups=total_groups,
                )
            )

            result.total_count = total_files
            processed_files = 0

            # ë””ë²„ê¹…: ì´ˆê¸° ìƒíƒœ ë¡œê¹…
            self.logger.info(
                f"ğŸš€ íŒŒì¼ ì •ë¦¬ ì‘ì—… ì‹œì‘ - ì´ íŒŒì¼: {total_files}ê°œ, ì´ ê·¸ë£¹: {total_groups}ê°œ"
            )

            # _processed_sources ì´ˆê¸°í™” (ì¤‘ìš”!)
            if not hasattr(result, "_processed_sources"):
                result._processed_sources = set()
            else:
                # ì´ì „ ì‹¤í–‰ì˜ ì”ì¬ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
                result._processed_sources.clear()

            self.logger.info(
                f"ğŸ“Š ì´ˆê¸°í™”ëœ _processed_sources ìƒíƒœ: {len(result._processed_sources)}ê°œ"
            )
            print(f"ğŸ” DEBUG: ì´ˆê¸°í™”ëœ _processed_sources ìƒíƒœ: {len(result._processed_sources)}ê°œ")
            print("=" * 50)
            print("ğŸ” DEBUG: íŒŒì¼ ì •ë¦¬ ì‹œì‘!")
            print(f"ğŸ” DEBUG: ì´ íŒŒì¼ ìˆ˜: {total_files}")
            print(f"ğŸ” DEBUG: ì´ ê·¸ë£¹ ìˆ˜: {total_groups}")
            print(f"ğŸ” DEBUG: _processed_sources ì´ˆê¸°í™”ë¨: {len(result._processed_sources)}")
            print("=" * 50)

            # ë””ë²„ê¹…: ê·¸ë£¹ ê°„ íŒŒì¼ ì¤‘ë³µ ê²€ì‚¬
            self._check_file_duplicates_across_groups()

            # ê° ê·¸ë£¹ë³„ë¡œ íŒŒì¼ ì •ë¦¬ (ìµœì í™”ëœ ì²˜ë¦¬)
            for _group_index, (group_name, group_data) in enumerate(self.grouped_items.items()):
                if self._cancelled:
                    break

                try:
                    self.logger.info(f"ğŸ“ ê·¸ë£¹ ì²˜ë¦¬ ì¤‘: {group_name}")

                    # ê·¸ë£¹ ë‚´ íŒŒì¼ë“¤ í™”ì§ˆë³„ ë¶„ë¥˜
                    files = group_data.get("files", [])
                    if not files:
                        continue

                    # ë””ë²„ê¹…: ê·¸ë£¹ ë‚´ íŒŒì¼ ëª©ë¡ ë¡œê¹…
                    self.logger.debug(f"ğŸ“‹ ê·¸ë£¹ '{group_name}' íŒŒì¼ ëª©ë¡:")
                    for file_data in files:
                        source_path = file_data.get("source_path", "")
                        self.logger.debug(f"   - {source_path}")

                    # ìœ íš¨í•œ íŒŒì¼ë§Œ í•„í„°ë§ (ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ì œì™¸)
                    valid_files = []
                    skipped_in_group = []
                    for file_data in files:
                        source_path = file_data.get("source_path", "")
                        if not source_path:
                            continue

                        # ê²½ë¡œ ì •ê·œí™” (Windows ê²½ë¡œ ë¬¸ì œ í•´ê²°)
                        normalized_path = str(Path(source_path))

                        if normalized_path in result._processed_sources:
                            self.logger.warning(
                                f"âš ï¸ ê·¸ë£¹ '{group_name}'ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆëœ€: {source_path}"
                            )
                            skipped_in_group.append(source_path)
                        else:
                            valid_files.append(file_data)

                    # ë””ë²„ê¹…: í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
                    self.logger.info(
                        f"ğŸ“Š ê·¸ë£¹ '{group_name}' í•„í„°ë§ ê²°ê³¼: ì›ë³¸ {len(files)}ê°œ â†’ ìœ íš¨ {len(valid_files)}ê°œ â†’ ê±´ë„ˆëœ€ {len(skipped_in_group)}ê°œ"
                    )
                    if skipped_in_group:
                        self.logger.info(f"â­ï¸ ê±´ë„ˆëœ€ íŒŒì¼ë“¤: {skipped_in_group}")

                    if not valid_files:
                        self.logger.info(f"â­ï¸ ê·¸ë£¹ '{group_name}'ì˜ ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë¨")
                        continue

                    # ë””ë²„ê¹…: ê·¸ë£¹ ì²˜ë¦¬ ì „ ìƒíƒœ ìƒì„¸ ë¡œê¹…
                    self.logger.info(f"ğŸ“Š ê·¸ë£¹ '{group_name}' ì²˜ë¦¬ ì „ ìƒíƒœ:")
                    self.logger.info(f"   - _processed_sources: {len(result._processed_sources)}ê°œ")
                    self.logger.info(f"   - valid_files: {len(valid_files)}ê°œ")
                    print(
                        f"ğŸ” DEBUG: ê·¸ë£¹ '{group_name}' ì²˜ë¦¬ ì „ - _processed_sources: {len(result._processed_sources)}ê°œ, valid_files: {len(valid_files)}ê°œ"
                    )

                    # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
                    existing_files = 0
                    missing_files = 0
                    for file_data in valid_files:
                        source_path = file_data.get("source_path", "")
                        if Path(source_path).exists():
                            existing_files += 1
                        else:
                            missing_files += 1
                            self.logger.warning(
                                f"âš ï¸ ê·¸ë£¹ '{group_name}' íŒŒì¼ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {source_path}"
                            )
                            print(f"ğŸ” DEBUG: íŒŒì¼ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {source_path}")

                    self.logger.info(
                        f"   - ì‹¤ì œ ì¡´ì¬ íŒŒì¼: {existing_files}ê°œ, ëˆ„ë½ íŒŒì¼: {missing_files}ê°œ"
                    )
                    print(
                        f"ğŸ” DEBUG: ê·¸ë£¹ '{group_name}' - ì¡´ì¬: {existing_files}ê°œ, ëˆ„ë½: {missing_files}ê°œ"
                    )

                    if missing_files > 0:
                        self.logger.warning(
                            f"ğŸš¨ ê·¸ë£¹ '{group_name}'ì— {missing_files}ê°œ íŒŒì¼ì´ ë””ìŠ¤í¬ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
                        )
                        print(f"ğŸ” DEBUG: ê·¸ë£¹ '{group_name}'ì— {missing_files}ê°œ íŒŒì¼ ëˆ„ë½ë¨")

                    # í™”ì§ˆë³„ë¡œ íŒŒì¼ ë¶„ë¥˜ ë° ì‹œì¦Œë³„ ì •ë¦¬ (ìŠ¤ë§ˆíŠ¸ ë¶„ë¥˜)
                    high_quality_files = []
                    low_quality_files = []

                    # íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê°€ì¥ ë†’ì€ í•´ìƒë„ë¥¼ ê³ í™”ì§ˆë¡œ ì²˜ë¦¬
                    file_groups = self._group_files_by_name(valid_files)

                    for base_name, files_in_group in file_groups.items():
                        if len(files_in_group) == 1:
                            # ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê³ í™”ì§ˆë¡œ ì·¨ê¸‰
                            high_quality_files.append(files_in_group[0])
                            self.logger.debug(f"ë‹¨ì¼ íŒŒì¼ ê³ í™”ì§ˆ ì²˜ë¦¬: {base_name}")
                        else:
                            # ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´ ê°€ì¥ ë†’ì€ í•´ìƒë„ë¥¼ ê³ í™”ì§ˆë¡œ, ë‚˜ë¨¸ì§€ëŠ” ì €í™”ì§ˆë¡œ
                            best_file = self._find_best_quality_file(files_in_group)
                            high_quality_files.append(best_file)

                            # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ì€ ì €í™”ì§ˆë¡œ
                            for file_data in files_in_group:
                                if file_data != best_file:
                                    low_quality_files.append(file_data)

                            self.logger.debug(
                                f"ë‹¤ì¤‘ íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ: {base_name} - ê³ í™”ì§ˆ: {Path(best_file['source_path']).name}, ì €í™”ì§ˆ: {len(files_in_group) - 1}ê°œ"
                            )

                    self.logger.info(
                        f"ğŸ” ê·¸ë£¹ '{group_name}' í™”ì§ˆë³„ ë¶„ë¥˜: ê³ í™”ì§ˆ {len(high_quality_files)}ê°œ, ì €í™”ì§ˆ {len(low_quality_files)}ê°œ (ì´ {len(valid_files)}ê°œ ìœ íš¨)"
                    )
                    self.logger.debug(
                        f"ğŸ“Š _processed_sources ìƒíƒœ: {len(result._processed_sources)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨"
                    )

                    # ê³ í™”ì§ˆ íŒŒì¼ë“¤ì„ ì‹œì¦Œë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ë°°ì¹˜
                    if high_quality_files:
                        self.logger.info(f"ğŸ¯ ê³ í™”ì§ˆ íŒŒì¼ë“¤ ì²˜ë¦¬ ì‹œì‘: {len(high_quality_files)}ê°œ")
                        # ì‹œì¦Œë³„ë¡œ íŒŒì¼ ë¶„ë¥˜ (ì§ê´€ì ì´ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬)
                        season_files = {}
                        for file_data in high_quality_files:
                            season = file_data.get("season", 1)

                            if season not in season_files:
                                season_files[season] = []
                            season_files[season].append(file_data)

                        # ê° ì‹œì¦Œë³„ë¡œ ë””ë ‰í† ë¦¬ ìƒì„± ë° íŒŒì¼ ì²˜ë¦¬
                        for season, season_file_list in season_files.items():
                            season_dir = (
                                self.destination_directory
                                / self._sanitize_filename(group_name)
                                / f"Season{season:02d}"
                            )
                            if not self.dry_run:
                                season_dir.mkdir(parents=True, exist_ok=True)
                                result.created_directories.append(season_dir)

                            # í•´ë‹¹ ì‹œì¦Œì˜ íŒŒì¼ë“¤ ì²˜ë¦¬
                            for file_data in season_file_list:
                                source_path = file_data.get("source_path", "")
                                self.logger.debug(f"ğŸ”„ ê³ í™”ì§ˆ íŒŒì¼ ì²˜ë¦¬ ì‹œë„: {source_path}")
                                success = self._organize_single_file(file_data, season_dir, result)
                                if success:
                                    result.success_count += 1
                                    self.logger.info(f"âœ… ê³ í™”ì§ˆ íŒŒì¼ ì´ë™ ì™„ë£Œ: {source_path}")
                                else:
                                    result.error_count += 1
                                    self.logger.warning(f"âŒ ê³ í™”ì§ˆ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {source_path}")
                                processed_files += 1

                    # ì €í™”ì§ˆ íŒŒì¼ë“¤ì„ ì‹œì¦Œë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ '_low res/' ì„œë¸Œë””ë ‰í† ë¦¬ì— ë°°ì¹˜
                    if low_quality_files:
                        self.logger.info(f"ğŸ¯ ì €í™”ì§ˆ íŒŒì¼ë“¤ ì²˜ë¦¬ ì‹œì‘: {len(low_quality_files)}ê°œ")
                        # ì‹œì¦Œë³„ë¡œ íŒŒì¼ ë¶„ë¥˜ (ì§ê´€ì ì´ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬)
                        season_files = {}
                        for file_data in low_quality_files:
                            season = file_data.get("season", 1)

                            if season not in season_files:
                                season_files[season] = []
                            season_files[season].append(file_data)

                        # ê° ì‹œì¦Œë³„ë¡œ ë””ë ‰í† ë¦¬ ìƒì„± ë° íŒŒì¼ ì²˜ë¦¬
                        for season, season_file_list in season_files.items():
                                season_dir = (
                                    self.destination_directory
                                    / self._sanitize_filename(group_name)
                                    / "_low res"
                                    / f"Season{season:02d}"
                                )
                                if not self.dry_run:
                                    season_dir.mkdir(parents=True, exist_ok=True)
                                    result.created_directories.append(season_dir)

                                # í•´ë‹¹ ì‹œì¦Œì˜ íŒŒì¼ë“¤ ì²˜ë¦¬
                                for file_data in season_file_list:
                                    source_path = file_data.get("source_path", "")
                                    self.logger.debug(f"ğŸ”„ ì €í™”ì§ˆ íŒŒì¼ ì²˜ë¦¬ ì‹œë„: {source_path}")
                                    success = self._organize_single_file(
                                        file_data, season_dir, result
                                    )
                                    if success:
                                        result.success_count += 1
                                        self.logger.info(f"âœ… ì €í™”ì§ˆ íŒŒì¼ ì´ë™ ì™„ë£Œ: {source_path}")
                                    else:
                                        result.error_count += 1
                                        self.logger.warning(
                                            f"âŒ ì €í™”ì§ˆ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {source_path}"
                                        )
                                    processed_files += 1
                    else:
                        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ê·¸ë£¹ ë””ë ‰í† ë¦¬ë§Œ ìƒì„±
                        group_dir = self.destination_directory / self._sanitize_filename(group_name)
                        if not self.dry_run:
                            group_dir.mkdir(parents=True, exist_ok=True)
                            result.created_directories.append(group_dir)

                    # ì§„í–‰ë¥  ì´ë²¤íŠ¸ ë°œí–‰
                    progress_percent = int((processed_files / total_files) * 100)
                    self.event_bus.publish(
                        OrganizationProgressEvent(
                            organization_id=self.organization_id,
                            current_file=processed_files,
                            total_files=total_files,
                            current_group=group_name,
                            operation_type="copy" if not self.dry_run else "validate",
                            current_file_path=Path(),  # ê·¸ë£¹ ë‹¨ìœ„ ì§„í–‰ë¥ ì´ë¯€ë¡œ íŒŒì¼ ê²½ë¡œëŠ” ë¹„ì›€
                            progress_percent=progress_percent,
                        )
                    )

                    # ë””ë²„ê¹…: ê·¸ë£¹ ì²˜ë¦¬ í›„ _processed_sources ìƒíƒœ
                    self.logger.debug(
                        f"ğŸ“Š ê·¸ë£¹ '{group_name}' ì²˜ë¦¬ í›„ _processed_sources: {len(result._processed_sources)}ê°œ"
                    )

                except Exception as e:
                    self.logger.error(f"ê·¸ë£¹ ì²˜ë¦¬ ì‹¤íŒ¨: {group_name}: {e}")
                    result.errors.append(f"ê·¸ë£¹ '{group_name}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

            # ì™„ë£Œ ì²˜ë¦¬
            result.operation_duration_seconds = time.time() - start_time

            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê·¸
            total_processed = result.success_count + result.error_count + result.skip_count
            success_rate = (
                (result.success_count / total_processed * 100) if total_processed > 0 else 0
            )

            self.logger.info("ğŸ“Š íŒŒì¼ ì •ë¦¬ ìµœì¢… ê²°ê³¼:")
            self.logger.info(f"   âœ… ì„±ê³µ: {result.success_count}ê°œ")
            self.logger.info(f"   âŒ ì‹¤íŒ¨: {result.error_count}ê°œ")
            self.logger.info(f"   â­ï¸  ê±´ë„ˆëœ€: {result.skip_count}ê°œ")
            self.logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}% ({total_processed}ê°œ ì²˜ë¦¬)")
            self.logger.info(f"   ğŸ“ ìƒì„±ëœ ë””ë ‰í† ë¦¬: {len(result.created_directories)}ê°œ")
            self.logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {result.operation_duration_seconds:.2f}ì´ˆ")
            self.logger.info(
                f"   ğŸ“‹ _processed_sources ìµœì¢… ìƒíƒœ: {len(result._processed_sources)}ê°œ íŒŒì¼"
            )
            self.logger.info(f"   ğŸ“Š ì´ íŒŒì¼ ìˆ˜: {result.total_count}ê°œ")

            # ë””ë²„ê¹…: ê²°ê³¼ ê²€ì¦
            if result.total_count != total_processed:
                self.logger.warning(
                    f"âš ï¸ íŒŒì¼ ìˆ˜ ë¶ˆì¼ì¹˜: ì´ íŒŒì¼ {result.total_count}ê°œ vs ì²˜ë¦¬ëœ íŒŒì¼ {total_processed}ê°œ"
                )
            else:
                self.logger.info(f"âœ… íŒŒì¼ ìˆ˜ ì¼ì¹˜: ì´ {total_processed}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")

            if self._cancelled:
                self.event_bus.publish(
                    OrganizationCancelledEvent(
                        organization_id=self.organization_id,
                        cancellation_reason="ì‚¬ìš©ì ìš”ì²­",
                        partial_result=result,
                    )
                )
                return TaskResult(
                    task_id=self.task_id,
                    status=TaskStatus.CANCELLED,
                    success=False,
                    result_data={"result": result},
                )
            self.event_bus.publish(
                OrganizationCompletedEvent(
                    organization_id=self.organization_id,
                    result=result,
                    status=OrganizationStatus.ORGANIZATION_COMPLETED,
                )
            )
            return TaskResult(
                task_id=self.task_id,
                status=TaskStatus.COMPLETED,
                success=True,
                result_data={"result": result},
            )

        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")
            error_result = OrganizationResult(error_count=1, total_count=1)
            error_result.errors.append(str(e))

            self.event_bus.publish(
                OrganizationFailedEvent(
                    organization_id=self.organization_id,
                    error_type=OrganizationErrorType.UNKNOWN_ERROR,
                    error_message=str(e),
                    partial_result=error_result,
                )
            )
            return TaskResult(
                task_id=self.task_id,
                status=TaskStatus.FAILED,
                success=False,
                result_data={"error": str(e)},
            )

    def cancel(self, reason: str = "ì‚¬ìš©ì ìš”ì²­") -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        self._cancelled = True
        self.logger.info(f"íŒŒì¼ ì •ë¦¬ ì‘ì—… ì·¨ì†Œ ìš”ì²­: {self.organization_id} (ì‚¬ìœ : {reason})")

    def _count_total_files(self, grouped_items: dict[str, Any]) -> int:
        """ì „ì²´ íŒŒì¼ ìˆ˜ ê³„ì‚°"""
        total = 0
        for group_data in grouped_items.values():
            files = group_data.get("files", [])
            total += len(files)
        return total

    def _check_file_duplicates_across_groups(self) -> None:
        """ê·¸ë£¹ ê°„ íŒŒì¼ ì¤‘ë³µ ê²€ì‚¬"""
        file_to_groups = {}
        total_duplicates = 0

        for group_name, group_data in self.grouped_items.items():
            files = group_data.get("files", [])
            for file_data in files:
                source_path = file_data.get("source_path", "")
                if source_path:
                    if source_path not in file_to_groups:
                        file_to_groups[source_path] = []
                    file_to_groups[source_path].append(group_name)

        # ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°
        for source_path, groups in file_to_groups.items():
            if len(groups) > 1:
                self.logger.warning(f"âš ï¸ íŒŒì¼ ì¤‘ë³µ ë°œê²¬: {source_path}")
                self.logger.warning(f"   ì†í•œ ê·¸ë£¹ë“¤: {groups}")
                total_duplicates += 1

        if total_duplicates > 0:
            self.logger.warning(f"ğŸš¨ ì´ {total_duplicates}ê°œ íŒŒì¼ì´ ì—¬ëŸ¬ ê·¸ë£¹ì— ì¤‘ë³µìœ¼ë¡œ ì†í•¨")
        else:
            self.logger.info("âœ… ê·¸ë£¹ ê°„ íŒŒì¼ ì¤‘ë³µ ì—†ìŒ")

    def _organize_single_file(
        self, file_data: dict[str, Any], group_dir: Path, result: OrganizationResult
    ) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ì •ë¦¬"""
        try:
            source_path = Path(file_data.get("source_path", ""))
            normalized_path = str(source_path)

            # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸ (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
            if (
                hasattr(result, "_processed_sources")
                and normalized_path in result._processed_sources
            ):
                self.logger.warning(f"ğŸ›‘ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆëœ€: {source_path}")
                self.logger.debug(
                    f"ğŸ“Š í˜„ì¬ _processed_sources í¬ê¸°: {len(result._processed_sources)}"
                )
                result.skip_count += 1
                result.skipped_files.append(str(source_path))
                return True

            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìºì‹œëœ ê²°ê³¼ í™œìš©)
            if not source_path.exists():
                # íŒŒì¼ì´ ì´ë¯¸ ì´ë™ë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                self.logger.debug(f"ğŸ›‘ íŒŒì¼ì´ ì´ë¯¸ ì´ë™ë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {source_path}")
                print(f"ğŸ” DEBUG: íŒŒì¼ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - {source_path}")
                result.skip_count += 1
                result.skipped_files.append(str(source_path))
                # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ ì¬ì²˜ë¦¬ ë°©ì§€
                if not hasattr(result, "_processed_sources"):
                    result._processed_sources = set()
                result._processed_sources.add(normalized_path)
                print(f"ğŸ” DEBUG: _processed_sourcesì— ì¶”ê°€ë¨: {normalized_path}")
                return True

            # ëŒ€ìƒ íŒŒì¼ëª… ìƒì„±
            target_filename = self._generate_target_filename(file_data)
            target_path = group_dir / target_filename

            # íŒŒì¼ ë³µì‚¬/ì´ë™ (dry_runì´ ì•„ë‹Œ ê²½ìš°)
            if not self.dry_run:
                if target_path.exists():
                    # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                    target_path = self._generate_unique_filename(target_path)

                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(source_path, target_path)
                result.processed_files.append(target_path)

                # ì›ë³¸ íŒŒì¼ ì‚­ì œ (ë³µì‚¬ ì„±ê³µ í›„)
                try:
                    source_path.unlink()
                    self.logger.debug(f"ì›ë³¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {source_path}")
                except Exception as e:
                    self.logger.warning(f"ì›ë³¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {source_path} - {e}")

                # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€ (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
                if not hasattr(result, "_processed_sources"):
                    result._processed_sources = set()
                result._processed_sources.add(normalized_path)

            else:
                result.processed_files.append(target_path)
                # dry_run ëª¨ë“œì—ì„œë„ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                if not hasattr(result, "_processed_sources"):
                    result._processed_sources = set()
                result._processed_sources.add(normalized_path)

            return True

        except Exception as e:
            self.logger.error(f"ë‹¨ì¼ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {file_data}: {e}")
            result.errors.append(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return False

    def _generate_target_filename(self, file_data: dict[str, Any]) -> str:
        """ëŒ€ìƒ íŒŒì¼ëª… ìƒì„±"""
        # íŒŒì¼ ë°ì´í„°ì—ì„œ ì œëª©, ì—í”¼ì†Œë“œ, ì‹œì¦Œ ë“± ì •ë³´ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
        title = file_data.get("title", "Unknown")
        episode = file_data.get("episode", "")
        season = file_data.get("season", 1)  # ì‹œì¦Œ ì •ë³´ ì¶”ê°€
        source_path = Path(file_data.get("source_path", ""))

        if episode:
            filename = f"{title} - S{season:02d}E{episode:02d}{source_path.suffix}"
        else:
            filename = f"{title} - S{season:02d}{source_path.suffix}"

        return self._sanitize_filename(filename)

    def _generate_unique_filename(self, target_path: Path) -> Path:
        """ì¤‘ë³µ íŒŒì¼ëª…ì— ëŒ€í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±"""
        base = target_path.stem
        suffix = target_path.suffix
        parent = target_path.parent

        counter = 1
        while True:
            new_name = f"{base}_{counter}{suffix}"
            new_path = parent / new_name
            if not new_path.exists():
                return new_path
            counter += 1

    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ë¦¬ (ê¸ˆì§€ ë¬¸ì ì œê±°)"""
        forbidden_chars = '<>:"/\\|?*'
        for char in forbidden_chars:
            filename = filename.replace(char, "_")
        return filename.strip()


class FileOrganizationService(IFileOrganizationService):
    """íŒŒì¼ ì •ë¦¬ ì„œë¹„ìŠ¤ êµ¬í˜„"""

    def __init__(self, event_bus: TypedEventBus, background_task_service: IBackgroundTaskService):
        self.event_bus = event_bus
        self.background_task_service = background_task_service
        self.logger = logging.getLogger(self.__class__.__name__)
        self._active_organizations: dict[UUID, str] = {}  # organization_id -> background_task_id

        self.logger.info("FileOrganizationService ì´ˆê¸°í™” ì™„ë£Œ")

    def validate_organization_request(
        self, grouped_items: dict[str, Any], destination_directory: Path
    ) -> OrganizationValidationResult:
        """íŒŒì¼ ì •ë¦¬ ìš”ì²­ ê²€ì¦"""
        validation_id = uuid4()

        self.event_bus.publish(
            OrganizationValidationStartedEvent(
                organization_id=validation_id, destination_directory=destination_directory
            )
        )

        result = OrganizationValidationResult()

        try:
            # ê·¸ë£¹ ë°ì´í„° ê²€ì¦
            if not grouped_items:
                result.validation_errors.append("ì •ë¦¬í•  ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                result.has_grouped_items = True
                result.total_groups = len(grouped_items)
                result.total_files = sum(
                    len(group.get("files", [])) for group in grouped_items.values()
                )

            # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ê²€ì¦
            if (
                not destination_directory
                or str(destination_directory) == "."
                or str(destination_directory) == ""
            ):
                result.validation_errors.append("ëŒ€ìƒ ë””ë ‰í† ë¦¬ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            else:
                result.has_destination = True

                if not destination_directory.exists():
                    result.validation_errors.append("ëŒ€ìƒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                else:
                    result.destination_exists = True

                    # ì“°ê¸° ê¶Œí•œ í™•ì¸
                    try:
                        test_file = destination_directory / ".animsorter_write_test"
                        test_file.touch()
                        test_file.unlink()
                        result.destination_writable = True
                    except Exception:
                        result.validation_errors.append("ëŒ€ìƒ ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤")

            # íŒŒì¼ í¬ê¸° ì¶”ì •
            result.estimated_size_mb = self._estimate_total_size(grouped_items)

            # ì „ì²´ ê²€ì¦ ê²°ê³¼
            result.is_valid = len(result.validation_errors) == 0

            self.event_bus.publish(
                OrganizationValidationCompletedEvent(
                    organization_id=validation_id, validation_result=result
                )
            )

            return result

        except Exception as e:
            self.logger.error(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.event_bus.publish(
                OrganizationValidationFailedEvent(
                    organization_id=validation_id,
                    error_type=OrganizationErrorType.VALIDATION_ERROR,
                    error_message=str(e),
                    validation_errors=[str(e)],
                )
            )

            result.validation_errors.append(str(e))
            return result

    def create_preflight_data(
        self, grouped_items: dict[str, Any], destination_directory: Path
    ) -> OrganizationPreflightData:
        """í”„ë¦¬í”Œë¼ì´íŠ¸ ë°ì´í„° ìƒì„±"""
        preflight_id = uuid4()

        try:
            self.event_bus.publish(OrganizationPreflightStartedEvent(organization_id=preflight_id))

            # ì˜ˆìƒ ì‘ì—… ìˆ˜ ê³„ì‚°
            estimated_operations = sum(
                len(group.get("files", [])) for group in grouped_items.values()
            )

            # ì˜ˆìƒ í¬ê¸° ê³„ì‚°
            estimated_size_mb = self._estimate_total_size(grouped_items)

            # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
            disk_space_available_mb = self._get_available_disk_space(destination_directory)

            # ìƒì„±ë  ë””ë ‰í† ë¦¬ ëª©ë¡
            will_create_directories = [
                destination_directory / self._sanitize_filename(group_name)
                for group_name in grouped_items
            ]

            # ì ì¬ì  ì¶©ëŒ ê²€ì‚¬
            potential_conflicts = self._check_potential_conflicts(
                grouped_items, destination_directory
            )

            preflight_data = OrganizationPreflightData(
                destination_directory=destination_directory,
                grouped_items=grouped_items,
                estimated_operations=estimated_operations,
                estimated_size_mb=estimated_size_mb,
                disk_space_available_mb=disk_space_available_mb,
                will_create_directories=will_create_directories,
                potential_conflicts=potential_conflicts,
            )

            self.event_bus.publish(
                OrganizationPreflightCompletedEvent(
                    organization_id=preflight_id,
                    user_approved=False,  # ì•„ì§ ì‚¬ìš©ì ìŠ¹ì¸ ì „
                    preflight_data=preflight_data,
                )
            )

            return preflight_data

        except Exception as e:
            self.logger.error(f"í”„ë¦¬í”Œë¼ì´íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
            return OrganizationPreflightData(
                destination_directory=destination_directory, grouped_items=grouped_items
            )

    def start_organization(
        self, grouped_items: dict[str, Any], destination_directory: Path, dry_run: bool = False
    ) -> UUID:
        """íŒŒì¼ ì •ë¦¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)"""
        organization_id = uuid4()
        self.logger.info(f"íŒŒì¼ ì •ë¦¬ ì‹œì‘: {destination_directory} (ì¡°ì§í™” ID: {organization_id})")

        # FileOrganizationTask ìƒì„±
        organization_task = FileOrganizationTask(
            organization_id=organization_id,
            grouped_items=grouped_items,
            destination_directory=destination_directory,
            event_bus=self.event_bus,
            dry_run=dry_run,
        )

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì œì¶œ
        background_task_id = self.background_task_service.submit_task(organization_task)
        self._active_organizations[organization_id] = background_task_id

        self.logger.info(
            f"íŒŒì¼ ì •ë¦¬ ì‘ì—… ì œì¶œë¨: {organization_id} (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ID: {background_task_id})"
        )
        return organization_id

    def cancel_organization(self, organization_id: UUID) -> bool:
        """íŒŒì¼ ì •ë¦¬ ì·¨ì†Œ"""
        if organization_id not in self._active_organizations:
            self.logger.warning(f"ì·¨ì†Œí•  íŒŒì¼ ì •ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {organization_id}")
            return False

        background_task_id = self._active_organizations[organization_id]
        success = self.background_task_service.cancel_task(background_task_id)

        if success:
            self.logger.info(f"íŒŒì¼ ì •ë¦¬ ì·¨ì†Œ ì„±ê³µ: {organization_id}")
            self._active_organizations.pop(organization_id)
        else:
            self.logger.warning(f"íŒŒì¼ ì •ë¦¬ ì·¨ì†Œ ì‹¤íŒ¨: {organization_id}")

        return success

    def get_organization_status(self, organization_id: UUID) -> OrganizationStatus | None:
        """íŒŒì¼ ì •ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        if organization_id in self._active_organizations:
            return OrganizationStatus.ORGANIZATION_PROGRESS
        return None

    def dispose(self) -> None:
        """ì„œë¹„ìŠ¤ ì •ë¦¬"""
        # ëª¨ë“  í™œì„± ì •ë¦¬ ì‘ì—… ì·¨ì†Œ
        for organization_id in list(self._active_organizations.keys()):
            self.cancel_organization(organization_id)

        self.logger.info("FileOrganizationService ì •ë¦¬ ì™„ë£Œ")

    # ===== í—¬í¼ ë©”ì„œë“œ =====

    def _estimate_total_size(self, grouped_items: dict[str, Any]) -> float:
        """ì „ì²´ ì˜ˆìƒ í¬ê¸° ê³„ì‚° (MB)"""
        total_size = 0.0
        for group_data in grouped_items.values():
            files = group_data.get("files", [])
            for file_data in files:
                source_path = Path(file_data.get("source_path", ""))
                try:
                    if source_path.exists():
                        total_size += source_path.stat().st_size
                except Exception:
                    pass  # íŒŒì¼ í¬ê¸°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ

        return total_size / (1024 * 1024)  # ë°”ì´íŠ¸ë¥¼ MBë¡œ ë³€í™˜

    def _get_available_disk_space(self, directory: Path) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ (MB)"""
        try:
            stat = shutil.disk_usage(directory)
            return stat.free / (1024 * 1024)  # ë°”ì´íŠ¸ë¥¼ MBë¡œ ë³€í™˜
        except Exception:
            return 0.0

    def _check_potential_conflicts(
        self, grouped_items: dict[str, Any], destination_directory: Path
    ) -> list[str]:
        """ì ì¬ì  ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        for group_name in grouped_items:
            group_dir = destination_directory / self._sanitize_filename(group_name)
            if group_dir.exists() and any(group_dir.iterdir()):
                conflicts.append(f"ê·¸ë£¹ ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•ŠìŒ: {group_name}")

        return conflicts

    def _group_files_by_name(self, files: list[dict[str, Any]]) -> dict[str, list[dict[str, Any]]]:
        """íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ë“¤ì„ ê·¸ë£¹í™”"""
        file_groups = {}

        for file_data in files:
            source_path = file_data.get("source_path", "")
            if not source_path:
                continue

            # íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ
            path_obj = Path(source_path)
            base_name = path_obj.stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…

            # ê·¸ë£¹ì— ì¶”ê°€
            if base_name not in file_groups:
                file_groups[base_name] = []
            file_groups[base_name].append(file_data)

        return file_groups

    def _find_best_quality_file(self, files: list[dict[str, Any]]) -> dict[str, Any]:
        """ê·¸ë£¹ ë‚´ì—ì„œ ê°€ì¥ ë†’ì€ í•´ìƒë„ì˜ íŒŒì¼ì„ ì°¾ìŒ"""
        if not files:
            return None

        if len(files) == 1:
            return files[0]

        # í•´ìƒë„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ (ë†’ì€ ìš°ì„ ìˆœìœ„ê°€ ë¨¼ì €)
        sorted_files = sorted(
            files,
            key=lambda f: self._get_resolution_priority(f.get("resolution", "")),
            reverse=True  # ë‚´ë¦¼ì°¨ìˆœ (ë†’ì€ ìš°ì„ ìˆœìœ„ê°€ ë¨¼ì €)
        )

        return sorted_files[0]  # ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„ì˜ íŒŒì¼ ë°˜í™˜

    def _get_resolution_priority(self, resolution: str) -> int:
        """í•´ìƒë„ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜ (ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)"""
        resolution_priority = {
            "4K": 100,
            "1440p": 90,
            "1080p": 80,
            "2K": 70,
            "720p": 60,
            "480p": 50,
        }

        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ìš°ì„ ìˆœìœ„ ë°˜í™˜
        resolution_upper = resolution.upper() if resolution else ""
        return resolution_priority.get(resolution_upper, 0)  # ê¸°ë³¸ê°’ 0 (ì•Œ ìˆ˜ ì—†ëŠ” í•´ìƒë„)

    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ë¦¬ (ê¸ˆì§€ ë¬¸ì ì œê±°)"""
        forbidden_chars = '<>:"/\\|?*'
        for char in forbidden_chars:
            filename = filename.replace(char, "_")
        return filename.strip()
