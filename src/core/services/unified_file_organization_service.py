"""
Unified File Organization Service

This service consolidates all file organization logic from various managers
and services to eliminate code duplication and provide a single source of truth
for file operations.
"""

import logging
import re

logger = logging.getLogger(__name__)
import shutil
import time
from collections.abc import Callable
from pathlib import Path
from typing import Any
from uuid import uuid4

from src.app.file_processing_events import (
    FileOperationType as ProcessingFileOperationType,
    FileProcessingProgressEvent,
    calculate_processing_speed,
    calculate_progress_percentage,
    estimate_remaining_time,
)
from src.core.backup.backup_manager import BackupPolicy, CentralizedBackupManager
from src.core.commands.file_operation_commands import (
    BatchFileOperationCommand,
    FileOperationCommand,
    FileOperationCommandInvoker,
)
from src.core.config.file_organization_config import FileOrganizationConfig
from src.core.file_parser import FileParser
from src.core.file_validation import FileValidator
from src.core.interfaces.file_organization_interface import (
    FileConflictResolution,
    FileOperationPlan,
    FileOperationResult,
    FileOperationType,
    FileScanResult,
    IFileBackupManager,
    IFileNamingStrategy,
    IFileOperationExecutor,
    IFileOrganizationService,
    IFileScanner,
)
from src.core.strategies.file_naming_strategies import NamingConfig, NamingStrategyFactory


class UnifiedFileScanner(IFileScanner):
    """Unified file scanning implementation"""

    def __init__(self, config: FileOrganizationConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger

    def scan_directory(
        self, directory_path: Path, recursive: bool = True, file_extensions: set[str] = None
    ) -> FileScanResult:
        """Scan directory for files matching criteria"""
        start_time = time.time()
        files_found: list[Path] = []
        errors = []
        try:
            if not directory_path.exists():
                errors.append(f"Directory does not exist: {directory_path}")
                return FileScanResult(files_found, 0, 0, errors)
            if file_extensions is None:
                file_extensions = self.config.video_extensions or set()
            pattern = "**/*" if recursive else "*"
            total_size = 0
            for file_path in directory_path.glob(pattern):
                if file_path.is_file() and file_path.suffix.lower() in file_extensions:
                    try:
                        file_size = file_path.stat().st_size
                        if file_size >= self.config.min_file_size:
                            files_found.append(file_path)
                            total_size += file_size
                    except OSError as e:
                        errors.append(f"Cannot access {file_path}: {e}")
            scan_duration = time.time() - start_time
            self.logger.info(
                f"Directory scan completed: {len(files_found)} files found in {scan_duration:.2f}s"
            )
            return FileScanResult(files_found, total_size, scan_duration, errors)
        except Exception as e:
            errors.append(f"Scan failed: {e}")
            self.logger.error(f"Directory scan failed: {e}")
            return FileScanResult(files_found, 0, time.time() - start_time, errors)

    def validate_file(self, file_path: Path) -> bool:
        """Validate if file is processable"""
        try:
            if not file_path.exists():
                return False
            if not file_path.is_file():
                return False
            file_size = file_path.stat().st_size
            if file_size < self.config.min_file_size:
                return False
            return len(str(file_path)) <= self.config.max_path_length
        except Exception as e:
            self.logger.error(f"File validation failed for {file_path}: {e}")
            return False


class StandardFileNamingStrategy(IFileNamingStrategy):
    """Standard file naming strategy implementation"""

    def __init__(self, config: FileOrganizationConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger

    def generate_target_path(
        self, source_path: Path, metadata: dict[str, Any], destination_root: Path
    ) -> Path:
        """Generate target path based on naming strategy"""
        try:
            title = metadata.get("title", "Unknown")
            season = metadata.get("season", 1)
            episode = metadata.get("episode", 1)
            resolution = metadata.get("resolution", "")
            safe_title = self._sanitize_filename(title)
            season_folder = f"Season{season:02d}"
            target_dir = destination_root / safe_title / season_folder
            filename = f"{safe_title} - S{season:02d}E{episode:02d}"
            if resolution and resolution != "Unknown":
                filename += f" [{resolution}]"
            filename += source_path.suffix
            return target_dir / filename
        except Exception as e:
            self.logger.error(f"Target path generation failed: {e}")
            return destination_root / source_path.name

    def resolve_conflict(self, target_path: Path, resolution: FileConflictResolution) -> Path:
        """Resolve file naming conflicts"""
        if not target_path.exists():
            return target_path
        if (
            resolution == FileConflictResolution.SKIP
            or resolution == FileConflictResolution.OVERWRITE
        ):
            return target_path
        if resolution == FileConflictResolution.RENAME:
            counter = 1
            while True:
                stem = target_path.stem
                suffix = target_path.suffix
                new_name = f"{stem}_{counter}{suffix}"
                new_path = target_path.parent / new_name
                if not new_path.exists():
                    return new_path
                counter += 1
        elif resolution == FileConflictResolution.BACKUP_AND_OVERWRITE:
            backup_path = target_path.with_suffix(f"{target_path.suffix}.backup_{int(time.time())}")
            try:
                shutil.copy2(target_path, backup_path)
                self.logger.info(f"Backup created: {backup_path}")
            except Exception as e:
                self.logger.error(f"Backup creation failed: {e}")
            return target_path
        return target_path

    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for filesystem compatibility"""
        invalid_chars = ["<", ">", ":", '"', "/", "\\", "|", "?", "*"]
        sanitized = filename
        for char in invalid_chars:
            sanitized = sanitized.replace(char, "_")
        sanitized = " ".join(sanitized.split())
        sanitized = sanitized.strip()
        if not sanitized:
            sanitized = "Unknown"
        return sanitized


class UnifiedFileOperationExecutor(IFileOperationExecutor):
    """Unified file operation execution implementation"""

    def __init__(
        self,
        config: FileOrganizationConfig,
        logger: logging.Logger,
        naming_strategy: IFileNamingStrategy,
        service=None,
    ):
        self.config = config
        self.logger = logger
        self.naming_strategy = naming_strategy
        self.service = service
        self.operation_id = uuid4()
        self.start_time: float | None = None
        self.processed_bytes = 0
        self.total_bytes = 0
        self.success_count = 0
        self.error_count = 0
        self.skip_count = 0

    def execute_operation(self, plan: FileOperationPlan) -> FileOperationResult:
        """Execute a single file operation"""
        start_time = time.time()
        try:
            if not plan.source_path.exists():
                return FileOperationResult(
                    success=False,
                    source_path=str(plan.source_path),
                    destination_path=str(plan.target_path),
                    operation_type=plan.operation_type.value,
                    error_message="Source file does not exist",
                )
            if self.config.create_directories:
                plan.target_path.parent.mkdir(parents=True, exist_ok=True)
            if plan.target_path.exists() and not self.config.overwrite_existing:
                plan.target_path = self.naming_strategy.resolve_conflict(
                    plan.target_path, FileConflictResolution.RENAME
                )
                logger.info("üîç Ìï¥ÏÉÅÎèÑ Ï∂îÏ∂ú ÏãúÎèÑ: %s", plan.source_path.name)
                logger.info("  ‚úÖ Ìå®ÌÑ¥ Îß§Ïπ≠: (Ï∂©Îèå Ìï¥Í≤∞) -> %s", plan.target_path.name)
            backup_path = None
            # ÎåÄÏÉÅ ÌååÏùºÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎ©¥ Î∞±ÏóÖ ÎòêÎäî ÏÇ≠Ï†ú Ï≤òÎ¶¨
            if plan.target_path.exists():
                if self.config.backup_before_operation:
                    backup_path = plan.target_path.with_suffix(
                        f"{plan.target_path.suffix}.backup_{int(time.time())}"
                    )
                    shutil.copy2(plan.target_path, backup_path)
                    logger.info("üíæ Í∏∞Ï°¥ ÌååÏùº Î∞±ÏóÖ: %s", backup_path.name)
                else:
                    # Î∞±ÏóÖÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞ Í∏∞Ï°¥ ÌååÏùº ÏÇ≠Ï†ú (Ïò§Î≤ÑÎùºÏù¥ÌåÖ)
                    plan.target_path.unlink()
                    logger.info("üîÑ Í∏∞Ï°¥ ÌååÏùº ÎçÆÏñ¥Ïì∞Í∏∞: %s", plan.target_path.name)

            if plan.operation_type == FileOperationType.COPY:
                shutil.copy2(plan.source_path, plan.target_path)
            elif plan.operation_type == FileOperationType.MOVE:
                shutil.move(plan.source_path, plan.target_path)
            elif plan.operation_type == FileOperationType.RENAME:
                plan.source_path.rename(plan.target_path)
            else:
                raise ValueError(f"Unsupported operation type: {plan.operation_type}")

            # ÏûêÎßâ ÌååÏùºÎèÑ Ìï®Íªò Ï≤òÎ¶¨ (UnifiedFileOrganizationServiceÏùò Î©îÏÑúÎìú Ìò∏Ï∂ú)
            if hasattr(self, "service") and hasattr(self.service, "_process_subtitle_files"):
                self.service._process_subtitle_files(plan.source_path, plan.target_path)

            actual_size = plan.target_path.stat().st_size if plan.target_path.exists() else 0
            processing_time = time.time() - start_time
            self.logger.info(
                f"File operation completed: {plan.source_path.name} -> {plan.target_path.name}"
            )
            return FileOperationResult(
                success=True,
                source_path=str(plan.source_path),
                destination_path=str(plan.target_path),
                operation_type=plan.operation_type.value,
                backup_path=str(backup_path) if backup_path else None,
                file_size=actual_size,
                processing_time=processing_time,
            )
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"File operation failed: {e}")
            return FileOperationResult(
                success=False,
                source_path=str(plan.source_path),
                destination_path=str(plan.target_path),
                operation_type=plan.operation_type.value,
                error_message=str(e),
                processing_time=processing_time,
            )

    def execute_batch_operations(
        self,
        plans: list[FileOperationPlan],
        progress_callback: Callable[[int, int], None] | None = None,
        detailed_progress_callback: Callable[[FileProcessingProgressEvent], None] | None = None,
    ) -> list[FileOperationResult]:
        """Execute multiple file operations in batch with detailed progress tracking"""
        self.operation_id = uuid4()
        self.start_time = time.time()
        self.processed_bytes = 0
        self.total_bytes = sum(plan.estimated_size for plan in plans)
        self.success_count = 0
        self.error_count = 0
        self.skip_count = 0
        results = []
        total_plans = len(plans)
        for i, plan in enumerate(plans):
            progress_percentage = calculate_progress_percentage(i, total_plans)
            time_elapsed = time.time() - (self.start_time or 0)
            speed_mbps = calculate_processing_speed(self.processed_bytes, time_elapsed)
            remaining_time = estimate_remaining_time(
                self.processed_bytes, self.total_bytes, time_elapsed
            )
            if detailed_progress_callback:
                progress_event = FileProcessingProgressEvent(
                    operation_id=self.operation_id,
                    current_file_index=i,
                    total_files=total_plans,
                    current_file_path=plan.source_path,
                    current_file_size=plan.estimated_size,
                    bytes_processed=self.processed_bytes,
                    total_bytes=self.total_bytes,
                    progress_percentage=progress_percentage,
                    current_operation=(
                        ProcessingFileOperationType.COPY
                        if plan.operation_type.value == "copy"
                        else ProcessingFileOperationType.MOVE
                    ),
                    current_step=f"Processing {plan.source_path.name}",
                    processing_speed_mbps=speed_mbps,
                    estimated_remaining_seconds=remaining_time,
                    success_count=self.success_count,
                    error_count=self.error_count,
                    skip_count=self.skip_count,
                )
                detailed_progress_callback(progress_event)
            result = self.execute_operation(plan)
            results.append(result)
            if result.success:
                self.success_count += 1
                self.processed_bytes += plan.estimated_size
            else:
                self.error_count += 1
            if progress_callback:
                progress_callback(i + 1, total_plans)
        return results

    def simulate_operations(self, plans: list[FileOperationPlan]) -> dict[str, Any]:
        """Simulate file operations without executing them"""
        simulation_results: dict[str, Any] = {
            "total_plans": len(plans),
            "successful": 0,
            "conflicts": 0,
            "errors": 0,
            "total_size": 0,
            "details": [],
        }
        for plan in plans:
            detail: dict[str, Any] = {
                "source": str(plan.source_path),
                "target": str(plan.target_path),
                "operation": plan.operation_type.value,
                "status": "success",
                "issues": [],
            }
            if plan.target_path.exists():
                detail["status"] = "conflict"
                detail["issues"].append("Target file already exists")
                simulation_results["conflicts"] += 1
            else:
                simulation_results["successful"] += 1
            if not plan.source_path.exists():
                detail["status"] = "error"
                detail["issues"].append("Source file does not exist")
                simulation_results["errors"] += 1
            if plan.source_path.exists():
                try:
                    file_size = plan.source_path.stat().st_size
                    simulation_results["total_size"] += file_size
                    detail["size"] = file_size
                except OSError:
                    detail["issues"].append("Cannot access source file")
            simulation_results["details"].append(detail)
        return simulation_results


class UnifiedFileBackupManager(IFileBackupManager):
    """Unified file backup management implementation"""

    def __init__(self, backup_root: Path, config: FileOrganizationConfig, logger: logging.Logger):
        self.backup_root = backup_root
        self.config = config
        self.logger = logger

    def create_backup(self, file_path: Path) -> Path:
        """Create backup of a file"""
        try:
            timestamp = int(time.time())
            backup_name = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
            backup_path = self.backup_root / backup_name
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file_path, backup_path)
            self.logger.info(f"Backup created: {backup_path}")
            return backup_path
        except Exception as e:
            self.logger.error(f"Backup creation failed for {file_path}: {e}")
            raise

    def restore_backup(self, backup_path: Path, target_path: Path) -> bool:
        """Restore file from backup"""
        try:
            if not backup_path.exists():
                self.logger.error(f"Backup file does not exist: {backup_path}")
                return False
            target_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(backup_path, target_path)
            self.logger.info(f"File restored from backup: {target_path}")
            return True
        except Exception as e:
            self.logger.error(f"Backup restore failed: {e}")
            return False

    def cleanup_old_backups(self, max_age_days: int = 30) -> int:
        """Clean up old backup files"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_days * 24 * 60 * 60
            cleaned_count = 0
            for backup_file in self.backup_root.rglob("*"):
                if backup_file.is_file():
                    file_age = current_time - backup_file.stat().st_mtime
                    if file_age > max_age_seconds:
                        backup_file.unlink()
                        cleaned_count += 1
            self.logger.info(f"Cleaned up {cleaned_count} old backup files")
            return cleaned_count
        except Exception as e:
            self.logger.error(f"Backup cleanup failed: {e}")
            return 0


class UnifiedFileOrganizationService(IFileOrganizationService):
    """Unified file organization service implementation with Command pattern support"""

    def __init__(self, config: FileOrganizationConfig = None, logger: logging.Logger = None):
        self.config = config or FileOrganizationConfig()
        self.logger = logger or logging.getLogger(__name__)
        self.scanner = UnifiedFileScanner(self.config, self.logger)
        self.naming_strategy = NamingStrategyFactory.create_strategy(
            self.config.naming_strategy, self.config.naming_config, self.logger
        )
        self.operation_executor = UnifiedFileOperationExecutor(
            self.config, self.logger, self.naming_strategy, self
        )
        self.backup_manager = CentralizedBackupManager(
            Path("_backup"), self.config.backup_policy, self.logger
        )
        self.file_parser = FileParser()
        self.file_validator = FileValidator()
        self.command_invoker = FileOperationCommandInvoker(self.logger)
        self.logger.info("UnifiedFileOrganizationService initialized with Command pattern support")

    def scan_and_plan_organization(
        self,
        source_directory: Path,
        destination_root: Path,
        naming_strategy: str = "standard",
        operation_type: FileOperationType = FileOperationType.COPY,
        grouped_items: dict = None,
    ) -> list[FileOperationPlan]:
        """Scan directory and create organization plan"""
        try:
            scan_result = self.scanner.scan_directory(source_directory)
            if not scan_result.files_found:
                self.logger.warning("No files found for organization")
                return []
            plans = []
            for file_path in scan_result.files_found:
                try:
                    parsed_metadata = self.file_parser.parse_filename(str(file_path))
                    if not parsed_metadata or not parsed_metadata.get("title"):
                        self.logger.warning(f"Could not parse metadata for {file_path}")
                        continue
                    # Ïï†ÎãàÎ©îÏù¥ÏÖò Ï†úÎ™©Í≥º ÏãúÏ¶å Ìè¥Îçî Íµ¨Ï°∞ ÏÉùÏÑ±
                    title = parsed_metadata.get("title") or file_path.stem
                    season = parsed_metadata.get("season") or 1

                    # grouped_itemsÏóêÏÑú TMDB Îß§Ïπò Ï†ïÎ≥¥ Ï∞æÍ∏∞
                    if grouped_items:
                        for group_items in grouped_items.values():
                            if isinstance(group_items, list):
                                for item in group_items:
                                    if (
                                        hasattr(item, "sourcePath")
                                        and Path(item.sourcePath) == file_path
                                    ):
                                        if (
                                            hasattr(item, "tmdbMatch")
                                            and item.tmdbMatch
                                            and item.tmdbMatch.name
                                        ):
                                            title = item.tmdbMatch.name
                                            self.logger.info(f"‚úÖ TMDB Îß§Ïπò Ï†úÎ™© ÏÇ¨Ïö©: {title}")
                                        if hasattr(item, "season") and item.season:
                                            season = item.season
                                        break
                    safe_title = re.sub(r"[^\w\sÍ∞Ä-Ìû£]", "", title).strip()
                    safe_title = re.sub(r"\s+", " ", safe_title)
                    if not safe_title:
                        safe_title = "Unknown"

                    season_folder = f"Season{season:02d}"
                    target_path = destination_root / safe_title / season_folder / file_path.name
                    # ÌååÏã±Îêú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©
                    metadata = {
                        "title": safe_title,
                        "season": season,
                        "episode": parsed_metadata.get("episode") or 1,
                        "resolution": parsed_metadata.get("resolution") or "Unknown",
                        "year": parsed_metadata.get("year"),
                        "group": parsed_metadata.get("group") or "Unknown",
                    }

                    # ÌååÏùºÎ™Ö Ï∂©Îèå Ìï¥Í≤∞
                    target_path = self.naming_strategy.resolve_conflict(
                        target_path, FileConflictResolution.RENAME
                    )
                    plan = FileOperationPlan(
                        source_path=file_path,
                        target_path=target_path,
                        operation_type=operation_type,
                        estimated_size=file_path.stat().st_size,
                        metadata=metadata,
                    )
                    plans.append(plan)
                except Exception as e:
                    self.logger.error(f"Failed to create plan for {file_path}: {e}")
            self.logger.info(f"Created {len(plans)} organization plans")
            return plans
        except Exception as e:
            self.logger.error(f"Organization planning failed: {e}")
            return []

    def execute_organization_plan(
        self,
        plans: list[FileOperationPlan],
        dry_run: bool = True,
        progress_callback: Callable[[int, int], None] | None = None,
        detailed_progress_callback: Callable[[FileProcessingProgressEvent], None] | None = None,
    ) -> list[FileOperationResult]:
        """Execute file organization plan with detailed progress tracking"""
        if dry_run:
            self.logger.info("Executing organization plan in dry-run mode")
            # dry_run Î™®ÎìúÏóêÏÑúÎäî Îπà Í≤∞Í≥º Î¶¨Ïä§Ìä∏ Î∞òÌôò
            return []
        self.logger.info("Executing organization plan")
        return self.operation_executor.execute_batch_operations(
            plans, progress_callback, detailed_progress_callback
        )

    def validate_organization_plan(self, plans: list[FileOperationPlan]) -> dict[str, Any]:
        """Validate organization plan for conflicts and issues"""
        validation_result: dict[str, Any] = {
            "valid": True,
            "total_plans": len(plans),
            "conflicts": 0,
            "errors": 0,
            "warnings": 0,
            "issues": [],
        }
        for i, plan in enumerate(plans):
            issues = []
            if not plan.source_path.exists():
                issues.append(f"Source file does not exist: {plan.source_path}")
                validation_result["errors"] += 1
            if plan.target_path.exists():
                issues.append(f"Target file already exists: {plan.target_path}")
                validation_result["conflicts"] += 1
            if len(str(plan.target_path)) > self.config.max_path_length:
                issues.append(f"Target path too long: {len(str(plan.target_path))} characters")
                validation_result["warnings"] += 1
            try:
                plan.target_path.parent.mkdir(parents=True, exist_ok=True)
            except PermissionError:
                issues.append(f"Cannot create target directory: {plan.target_path.parent}")
                validation_result["errors"] += 1
            if issues:
                validation_result["issues"].append(
                    {
                        "plan_index": i,
                        "source": str(plan.source_path),
                        "target": str(plan.target_path),
                        "issues": issues,
                    }
                )
        if validation_result["errors"] > 0 or validation_result["conflicts"] > 0:
            validation_result["valid"] = False
        return validation_result

    def get_organization_statistics(self, plans: list[FileOperationPlan]) -> dict[str, Any]:
        """Get statistics about organization plan"""
        total_size = sum(plan.estimated_size for plan in plans)
        operation_counts: dict[str, int] = {}
        for plan in plans:
            op_type = plan.operation_type.value
            operation_counts[op_type] = operation_counts.get(op_type, 0) + 1
        estimated_time = total_size / (100 * 1024 * 1024)
        return {
            "total_plans": len(plans),
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "operation_counts": operation_counts,
            "estimated_time_seconds": max(1, int(estimated_time)),
            "average_file_size_mb": total_size / len(plans) / (1024 * 1024) if plans else 0,
        }

    def execute_organization_with_commands(
        self,
        plans: list[FileOperationPlan],
        progress_callback: Callable[[int, int], None] | None = None,
        detailed_progress_callback: Callable[[FileProcessingProgressEvent], None] | None = None,
    ) -> dict[str, Any]:
        """Execute organization plan using Command pattern for undo/redo support with progress tracking"""
        try:
            operation_id = uuid4()
            total_plans = len(plans)
            commands = []
            for i, plan in enumerate(plans):
                command = FileOperationCommand(
                    plan.source_path, plan.target_path, plan.operation_type, self.logger
                )
                commands.append(command)
                if detailed_progress_callback:
                    progress_event = FileProcessingProgressEvent(
                        operation_id=operation_id,
                        current_file_index=i,
                        total_files=total_plans,
                        current_file_path=plan.source_path,
                        current_file_size=plan.estimated_size,
                        progress_percentage=calculate_progress_percentage(i, total_plans),
                        current_operation=(
                            ProcessingFileOperationType.COPY
                            if plan.operation_type.value == "copy"
                            else ProcessingFileOperationType.MOVE
                        ),
                        current_step="Preparing commands",
                    )
                    detailed_progress_callback(progress_event)
            batch_command = BatchFileOperationCommand(commands, self.logger)
            result = self.command_invoker.execute_command(batch_command)
            if progress_callback:
                progress_callback(len(plans), len(plans))
            if detailed_progress_callback:
                final_progress_event = FileProcessingProgressEvent(
                    operation_id=operation_id,
                    current_file_index=total_plans,
                    total_files=total_plans,
                    progress_percentage=100.0,
                    current_step="Command execution completed",
                )
                detailed_progress_callback(final_progress_event)
            return {
                "success": result.success,
                "message": result.message,
                "data": result.data,
                "error": result.error,
            }
        except Exception as e:
            self.logger.error(f"Command-based organization execution failed: {e}")
            return {
                "success": False,
                "message": f"Organization execution failed: {str(e)}",
                "error": str(e),
            }

    def undo_last_organization(self) -> dict[str, Any]:
        """Undo the last organization operation"""
        try:
            result = self.command_invoker.undo_last_command()
            return {"success": result.success, "message": result.message, "error": result.error}
        except Exception as e:
            self.logger.error(f"Undo operation failed: {e}")
            return {"success": False, "message": f"Undo failed: {str(e)}", "error": str(e)}

    def redo_last_organization(self) -> dict[str, Any]:
        """Redo the last undone organization operation"""
        try:
            result = self.command_invoker.redo_last_command()
            return {"success": result.success, "message": result.message, "error": result.error}
        except Exception as e:
            self.logger.error(f"Redo operation failed: {e}")
            return {"success": False, "message": f"Redo failed: {str(e)}", "error": str(e)}

    def can_undo(self) -> bool:
        """Check if undo is possible"""
        return self.command_invoker.can_undo()

    def can_redo(self) -> bool:
        """Check if redo is possible"""
        return self.command_invoker.can_redo()

    def get_command_history(self) -> list[str]:
        """Get command history descriptions"""
        return self.command_invoker.get_history_description()

    def clear_command_history(self):
        """Clear command history"""
        self.command_invoker.clear_history()
        self.logger.info("Command history cleared")

    def set_naming_strategy(self, strategy_name: str, config: NamingConfig = None):
        """Change the naming strategy"""
        try:
            self.naming_strategy = NamingStrategyFactory.create_strategy(
                strategy_name, config or self.config.naming_config, self.logger
            )
            self.config.naming_strategy = strategy_name
            if config:
                self.config.naming_config = config
            self.logger.info(f"Naming strategy changed to: {strategy_name}")
        except ValueError as e:
            self.logger.error(f"Failed to set naming strategy: {e}")
            raise

    def get_available_naming_strategies(self) -> list[str]:
        """Get list of available naming strategies"""
        return NamingStrategyFactory.get_available_strategies()

    def get_naming_strategy_description(self, strategy_name: str) -> str:
        """Get description of a naming strategy"""
        return NamingStrategyFactory.get_strategy_description(strategy_name)

    def get_current_naming_strategy(self) -> str:
        """Get current naming strategy name"""
        return self.config.naming_strategy

    def get_backup_stats(self) -> dict[str, Any]:
        """Get backup statistics"""
        return self.backup_manager.get_backup_stats()

    def list_backups(self, original_path: Path = None) -> list[Any]:
        """List all backups or backups for a specific file"""
        return self.backup_manager.list_backups(original_path)

    def cleanup_backups(self, max_age_days: int = None) -> int:
        """Clean up old backup files"""
        return self.backup_manager.cleanup_old_backups(max_age_days)

    def set_backup_policy(self, policy: BackupPolicy):
        """Update backup policy"""
        self.config.backup_policy = policy
        self.backup_manager = CentralizedBackupManager(Path("_backup"), policy, self.logger)
        self.logger.info("Backup policy updated")

    def _process_subtitle_files(self, source_path: Path, target_path: Path):
        """ÎπÑÎîîÏò§ ÌååÏùºÍ≥º Ïó∞Í¥ÄÎêú ÏûêÎßâ ÌååÏùºÎì§ÏùÑ Ìï®Íªò Ï≤òÎ¶¨Ìï©ÎãàÎã§"""
        try:
            # ÎπÑÎîîÏò§ ÌååÏùºÏù∏ÏßÄ ÌôïÏù∏
            if not self._is_video_file(source_path):
                return

            # ÏûêÎßâ ÌååÏùº Ï∞æÍ∏∞
            subtitle_files = self._find_subtitle_files(source_path)
            if not subtitle_files:
                return

            # ÏûêÎßâ ÌååÏùºÎì§ÏùÑ ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨Î°ú Ïù¥Îèô/Î≥µÏÇ¨
            for subtitle_path in subtitle_files:
                try:
                    subtitle_filename = Path(subtitle_path).name
                    subtitle_target_path = target_path.parent / subtitle_filename

                    # ÏûêÎßâ ÌååÏùºÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ Ï≤òÎ¶¨
                    if subtitle_target_path.exists():
                        if self.config.overwrite_existing:
                            # Ïò§Î≤ÑÎùºÏù¥ÌåÖ Î™®Îìú: Í∏∞Ï°¥ ÌååÏùº ÏÇ≠Ï†ú
                            subtitle_target_path.unlink()
                            self.logger.info(f"üîÑ Í∏∞Ï°¥ ÏûêÎßâ ÌååÏùº ÎçÆÏñ¥Ïì∞Í∏∞: {subtitle_filename}")
                        else:
                            # Ï∂©Îèå Ìï¥Í≤∞ Î™®Îìú: ÌååÏùºÎ™Ö Î≥ÄÍ≤Ω
                            subtitle_target_path = self.naming_strategy.resolve_conflict(
                                subtitle_target_path, FileConflictResolution.RENAME
                            )

                    # ÏûêÎßâ ÌååÏùº Ïù¥Îèô/Î≥µÏÇ¨
                    if self.config.backup_before_operation and subtitle_target_path.exists():
                        backup_path = subtitle_target_path.with_suffix(
                            f"{subtitle_target_path.suffix}.backup_{int(time.time())}"
                        )
                        shutil.copy2(subtitle_target_path, backup_path)
                        self.logger.info(f"üíæ Í∏∞Ï°¥ ÏûêÎßâ ÌååÏùº Î∞±ÏóÖ: {backup_path.name}")

                    shutil.move(subtitle_path, subtitle_target_path)
                    self.logger.info(f"‚úÖ ÏûêÎßâ ÌååÏùº Ï≤òÎ¶¨ ÏôÑÎ£å: {subtitle_filename}")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÏûêÎßâ ÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå®: {subtitle_path} - {e}")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏûêÎßâ ÌååÏùº Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

    def _is_video_file(self, file_path: Path) -> bool:
        """ÌååÏùºÏù¥ ÎπÑÎîîÏò§ ÌååÏùºÏù∏ÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§"""
        video_extensions = self.config.video_extensions or set()
        return file_path.suffix.lower() in video_extensions

    def _find_subtitle_files(self, video_path: Path) -> list[Path]:
        """ÎπÑÎîîÏò§ ÌååÏùºÍ≥º Ïó∞Í¥ÄÎêú ÏûêÎßâ ÌååÏùºÎì§ÏùÑ Ï∞æÏäµÎãàÎã§"""
        subtitle_files = []
        subtitle_extensions = self.config.subtitle_extensions or set()

        try:
            video_dir = video_path.parent
            video_basename = video_path.stem

            for file_path in video_dir.iterdir():
                if not file_path.is_file():
                    continue

                file_ext = file_path.suffix.lower()
                if file_ext not in subtitle_extensions:
                    continue

                subtitle_basename = file_path.stem
                if subtitle_basename == video_basename:
                    subtitle_files.append(file_path)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏûêÎßâ ÌååÏùº Í≤ÄÏÉâ Ï§ë Ïò§Î•ò: {e}")

        return subtitle_files

    def get_backup_policy(self) -> BackupPolicy:
        """Get current backup policy"""
        return self.config.backup_policy or BackupPolicy()

    def reset_state(self):
        """Reset the service state to its initial values.

        This method clears all accumulated state from previous operations
        and resets the service to a clean state.
        """
        try:
            logger.info("üîÑ Resetting UnifiedFileOrganizationService state...")

            # Reset operation executor state
            if hasattr(self.operation_executor, "operation_id"):
                self.operation_executor.operation_id = uuid4()
            if hasattr(self.operation_executor, "start_time"):
                self.operation_executor.start_time = None
            if hasattr(self.operation_executor, "processed_bytes"):
                self.operation_executor.processed_bytes = 0
            if hasattr(self.operation_executor, "total_bytes"):
                self.operation_executor.total_bytes = 0
            if hasattr(self.operation_executor, "success_count"):
                self.operation_executor.success_count = 0
            if hasattr(self.operation_executor, "error_count"):
                self.operation_executor.error_count = 0
            if hasattr(self.operation_executor, "skip_count"):
                self.operation_executor.skip_count = 0

            # Clear command history if available
            if hasattr(self.command_invoker, "clear_history"):
                self.command_invoker.clear_history()

            logger.info("‚úÖ UnifiedFileOrganizationService state reset completed")

        except Exception as e:
            logger.error(f"‚ùå Error resetting UnifiedFileOrganizationService state: {e}")
            import traceback

            traceback.print_exc()
