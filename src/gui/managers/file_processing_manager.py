"""
íŒŒì¼ ì²˜ë¦¬ ê´€ë¦¬ì
íŒŒì¼ ìŠ¤ìº”, íŒŒì‹±, ì •ë¦¬ ê³„íš ìˆ˜ë¦½ ë“±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
ë¦¬íŒ©í† ë§: í†µí•©ëœ íŒŒì¼ ì¡°ì§í™” ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì½”ë“œ ì œê±°
"""

import os
import shutil
import sys
import time
from collections.abc import Callable
from dataclasses import dataclass
from pathlib import Path
from uuid import uuid4

sys.path.append(str(Path(__file__).parent.parent.parent))

# Legacy FileManager import removed - using UnifiedFileOrganizationService instead
from src.app.file_processing_events import (FileOperationType,
                                            FileProcessingCompletedEvent,
                                            FileProcessingFailedEvent,
                                            FileProcessingProgressEvent,
                                            FileProcessingStartedEvent,
                                            calculate_progress_percentage)
from src.core.file_parser import FileParser
from src.core.interfaces.file_organization_interface import \
    FileConflictResolution
from src.core.services.unified_file_organization_service import (
    FileOperationPlan, FileOperationType, FileOrganizationConfig,
    UnifiedFileOrganizationService)
from src.core.video_metadata_extractor import VideoMetadataExtractor
from src.gui.managers.anime_data_manager import ParsedItem


@dataclass
class FileProcessingPlan:
    """íŒŒì¼ ì²˜ë¦¬ ê³„íš"""

    source_path: str
    target_path: str
    action: str  # 'move', 'copy', 'rename'
    backup_path: str | None = None
    estimated_size: int | None = None
    conflicts: list[str] = None

    def __post_init__(self):
        if self.conflicts is None:
            self.conflicts = []


@dataclass
class ProcessingStats:
    """ì²˜ë¦¬ í†µê³„"""

    total_files: int = 0
    processed_files: int = 0
    skipped_files: int = 0
    error_files: int = 0
    total_size_mb: int = 0
    estimated_time_seconds: int = 0


class FileProcessingManager:
    """íŒŒì¼ ì²˜ë¦¬ ê´€ë¦¬ì - ë¦¬íŒ©í† ë§ëœ ë²„ì „"""

    def __init__(self, destination_root: str = "", safe_mode: bool = True, event_bus=None):
        """ì´ˆê¸°í™”"""
        self.destination_root = destination_root
        self.safe_mode = safe_mode
        self.event_bus = event_bus
        self.current_operation_id = None

        # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        self.file_parser = FileParser()
        # Legacy FileManager removed - using UnifiedFileOrganizationService instead
        self.video_metadata_extractor = VideoMetadataExtractor()

        # í†µí•©ëœ íŒŒì¼ ì¡°ì§í™” ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        config = FileOrganizationConfig(
            safe_mode=safe_mode, backup_before_operation=safe_mode, overwrite_existing=not safe_mode
        )
        self.unified_service = UnifiedFileOrganizationService(config)

        # ì²˜ë¦¬ ê³„íš ì €ì¥ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        self.processing_plans: list[FileProcessingPlan] = []
        self.processing_stats = ProcessingStats()

        print("âœ… FileProcessingManager ì´ˆê¸°í™” ì™„ë£Œ (ë¦¬íŒ©í† ë§ëœ ë²„ì „)")

    def scan_directory(self, directory_path: str, recursive: bool = True) -> list[str]:
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº”í•˜ì—¬ ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° - ë¦¬íŒ©í† ë§ëœ ë²„ì „"""
        try:
            # í†µí•©ëœ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ìº”
            scan_result = self.unified_service.scanner.scan_directory(
                Path(directory_path), recursive=recursive
            )

            # ê²°ê³¼ë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€)
            video_files = [str(file_path) for file_path in scan_result.files_found]

            print(f"ğŸ” ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì™„ë£Œ: {len(video_files)}ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ë°œê²¬")
            if scan_result.errors:
                print(f"âš ï¸ ìŠ¤ìº” ì¤‘ {len(scan_result.errors)}ê°œ ì˜¤ë¥˜ ë°œìƒ")

            return video_files

        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []

    def parse_files(self, file_paths: list[str]) -> list[ParsedItem]:
        """íŒŒì¼ë“¤ì„ íŒŒì‹±í•˜ì—¬ ParsedItem ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        if not file_paths:
            return []

        print(f"ğŸ” íŒŒì¼ íŒŒì‹± ì‹œì‘: {len(file_paths)}ê°œ íŒŒì¼")

        # Emit processing started event
        self.current_operation_id = uuid4()
        if self.event_bus:
            started_event = FileProcessingStartedEvent(
                operation_id=self.current_operation_id,
                operation_type="file_parsing",
                total_files=len(file_paths),
                total_size_bytes=sum(
                    Path(fp).stat().st_size for fp in file_paths if Path(fp).exists()
                ),
                processing_mode="normal",
            )
            self.event_bus.publish(started_event)

        parsed_items = []
        for i, file_path in enumerate(file_paths):
            try:
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = int((i / len(file_paths)) * 100)
                print(f"ì§„í–‰ë¥ : {progress}% - {Path(file_path).name}")

                # Emit progress event
                if self.event_bus:
                    progress_event = FileProcessingProgressEvent(
                        operation_id=self.current_operation_id,
                        current_file_index=i,
                        total_files=len(file_paths),
                        current_file_path=Path(file_path),
                        current_file_size=(
                            Path(file_path).stat().st_size if Path(file_path).exists() else 0
                        ),
                        progress_percentage=calculate_progress_percentage(i, len(file_paths)),
                        current_operation=FileOperationType.PARSE,
                        current_step=f"Parsing {Path(file_path).name}",
                        success_count=len(
                            [item for item in parsed_items if item.status != "error"]
                        ),
                        error_count=len([item for item in parsed_items if item.status == "error"]),
                    )
                    self.event_bus.publish(progress_event)

                # íŒŒì¼ íŒŒì‹±
                parsed_metadata = self.file_parser.parse_filename(file_path)

                if parsed_metadata and parsed_metadata.title:
                    # íŒŒì¼ í¬ê¸° ê³„ì‚°
                    file_size = Path(file_path).stat().st_size
                    size_mb = file_size // (1024 * 1024)

                    # í•´ìƒë„ ì •ë³´ ì‚¬ìš© (TMDBì—ì„œ ì´ë¯¸ ê°€ì ¸ì˜¨ ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©)
                    resolution = parsed_metadata.resolution or "Unknown"

                    # ParsedItem ìƒì„±
                    parsed_item = ParsedItem(
                        sourcePath=file_path,
                        detectedTitle=parsed_metadata.title,
                        title=parsed_metadata.title,
                        season=parsed_metadata.season or 1,
                        episode=parsed_metadata.episode or 1,
                        resolution=resolution,
                        container=parsed_metadata.container or "Unknown",
                        codec=parsed_metadata.codec or "Unknown",
                        year=parsed_metadata.year,
                        group=parsed_metadata.group or "Unknown",
                        sizeMB=size_mb,
                        status="pending",
                        parsingConfidence=parsed_metadata.confidence or 0.0,
                    )
                    parsed_items.append(parsed_item)

                    print(
                        f"âœ… íŒŒì‹± ì„±ê³µ: {parsed_metadata.title} S{parsed_item.season:02d}E{parsed_item.episode:02d}"
                    )
                else:
                    # íŒŒì‹± ì‹¤íŒ¨
                    parsed_item = ParsedItem(
                        sourcePath=file_path,
                        detectedTitle="Unknown",
                        title="Unknown",
                        status="error",
                        parsingConfidence=0.0,
                    )
                    parsed_items.append(parsed_item)
                    print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {Path(file_path).name}")

            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {file_path} - {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œ
                parsed_item = ParsedItem(
                    sourcePath=file_path,
                    detectedTitle="Error",
                    title="Error",
                    status="error",
                    parsingConfidence=0.0,
                )
                parsed_items.append(parsed_item)

        print(f"âœ… íŒŒì¼ íŒŒì‹± ì™„ë£Œ: {len(parsed_items)}ê°œ ì•„ì´í…œ ìƒì„±")

        # Emit processing completed event
        if self.event_bus:
            completed_event = FileProcessingCompletedEvent(
                operation_id=self.current_operation_id,
                total_files=len(file_paths),
                successful_files=len([item for item in parsed_items if item.status != "error"]),
                failed_files=len([item for item in parsed_items if item.status == "error"]),
                skipped_files=0,
                total_size_bytes=sum(
                    Path(fp).stat().st_size for fp in file_paths if Path(fp).exists()
                ),
                processed_size_bytes=sum(
                    Path(fp).stat().st_size for fp in file_paths if Path(fp).exists()
                ),
                total_processing_time_seconds=0.0,  # Could be calculated if needed
                errors=[item.sourcePath for item in parsed_items if item.status == "error"],
            )
            self.event_bus.publish(completed_event)

        return parsed_items

    def create_processing_plans(
        self, parsed_items: list[ParsedItem], naming_scheme: str = "standard"
    ) -> list[FileProcessingPlan]:
        """íŒŒì¼ ì²˜ë¦¬ ê³„íš ìƒì„± - ë¦¬íŒ©í† ë§ëœ ë²„ì „"""
        if not parsed_items:
            return []

        print(f"ğŸ“‹ ì²˜ë¦¬ ê³„íš ìƒì„± ì‹œì‘: {len(parsed_items)}ê°œ ì•„ì´í…œ")

        try:
            # í†µí•©ëœ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„íš ìƒì„±
            source_paths = [
                Path(item.sourcePath) for item in parsed_items if item.status != "error"
            ]

            if not source_paths:
                print("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return []

            # í†µí•©ëœ ì„œë¹„ìŠ¤ë¡œ ì¡°ì§í™” ê³„íš ìƒì„±
            unified_plans = self.unified_service.scan_and_plan_organization(
                source_paths[0].parent,  # ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
                Path(self.destination_root),
                naming_scheme,
                FileOperationType.COPY if self.safe_mode else FileOperationType.MOVE,
            )

            # ê¸°ì¡´ FileProcessingPlan í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            self.processing_plans = []
            total_size = 0

            for unified_plan in unified_plans:
                # ParsedItemì—ì„œ í•´ë‹¹ íŒŒì¼ ì°¾ê¸°
                matching_item = None
                for item in parsed_items:
                    if Path(item.sourcePath) == unified_plan.source_path:
                        matching_item = item
                        break

                if not matching_item:
                    continue

                # ì¶©ëŒ í™•ì¸
                conflicts = self._check_conflicts(str(unified_plan.target_path))

                # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                plan = FileProcessingPlan(
                    source_path=str(unified_plan.source_path),
                    target_path=str(unified_plan.target_path),
                    action="copy" if self.safe_mode else "move",
                    backup_path=str(unified_plan.backup_path) if unified_plan.backup_path else None,
                    estimated_size=unified_plan.estimated_size // (1024 * 1024),  # MBë¡œ ë³€í™˜
                    conflicts=conflicts,
                )

                self.processing_plans.append(plan)
                total_size += unified_plan.estimated_size

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_stats.total_files = len(self.processing_plans)
            self.processing_stats.total_size_mb = total_size // (1024 * 1024)
            self.processing_stats.estimated_time_seconds = self._estimate_processing_time(
                total_size // (1024 * 1024)
            )

            print(f"âœ… ì²˜ë¦¬ ê³„íš ìƒì„± ì™„ë£Œ: {len(self.processing_plans)}ê°œ ê³„íš")
            return self.processing_plans

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ê³„íš ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def _generate_target_path(self, item: ParsedItem, naming_scheme: str) -> str:
        """ëŒ€ìƒ ê²½ë¡œ ìƒì„±"""
        if not self.destination_root:
            return item.sourcePath

        # ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        base_dir = (
            Path(self.destination_root)
            / (item.title or "Unknown")
            / (f"Season {item.season:02d}" if item.season else "Unknown")
        )

        # íŒŒì¼ëª… ìƒì„±
        if naming_scheme == "standard":
            filename = f"{item.title} - S{item.season:02d}E{item.episode:02d}"
        elif naming_scheme == "compact":
            filename = f"S{item.season:02d}E{item.episode:02d}"
        else:
            filename = Path(item.sourcePath).stem

        # í•´ìƒë„ ì •ë³´ ì¶”ê°€
        if item.resolution and item.resolution != "Unknown":
            filename += f" [{item.resolution}]"

        # í™•ì¥ì ì¶”ê°€
        extension = Path(item.sourcePath).suffix
        filename += extension

        # ì „ì²´ ê²½ë¡œ ìƒì„±
        return str(base_dir / filename)

    def _generate_backup_path(self, original_path: str) -> str:
        """ë°±ì—… ê²½ë¡œ ìƒì„±"""
        path = Path(original_path)
        backup_name = f"{path.stem}_backup_{int(time.time())}{path.suffix}"
        backup_path = path.parent / backup_name
        return str(backup_path)

    def _check_conflicts(self, target_path: str) -> list[str]:
        """íŒŒì¼ ì¶©ëŒ í™•ì¸"""
        conflicts = []

        if Path(target_path).exists():
            conflicts.append("íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•¨")

        # ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
        target_dir = Path(target_path).parent
        if not target_dir.exists() or not os.access(str(target_dir), os.W_OK):
            conflicts.append("ë””ë ‰í† ë¦¬ ì“°ê¸° ê¶Œí•œ ì—†ìŒ")

        return conflicts

    def _estimate_processing_time(self, total_size_mb: int) -> int:
        """ì²˜ë¦¬ ì‹œê°„ ì¶”ì • (ì´ˆ ë‹¨ìœ„)"""
        # í‰ê·  ì²˜ë¦¬ ì†ë„: 100MB/ì´ˆ (ë³µì‚¬ ê¸°ì¤€)
        estimated_seconds = total_size_mb // 100

        # ìµœì†Œ 1ì´ˆ
        return max(1, estimated_seconds)

    def simulate_processing(self) -> dict[str, any]:
        """íŒŒì¼ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
        if not self.processing_plans:
            return {"error": "ì²˜ë¦¬ ê³„íšì´ ì—†ìŠµë‹ˆë‹¤"}

        print("ğŸ­ íŒŒì¼ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")

        simulation_results = {
            "total_files": len(self.processing_plans),
            "total_size_mb": self.processing_stats.total_size_mb,
            "estimated_time": self.processing_stats.estimated_time_seconds,
            "conflicts": [],
            "success_count": 0,
            "error_count": 0,
        }

        for plan in self.processing_plans:
            if plan.conflicts:
                simulation_results["conflicts"].append(
                    {
                        "source": plan.source_path,
                        "target": plan.target_path,
                        "conflicts": plan.conflicts,
                    }
                )
                simulation_results["error_count"] += 1
            else:
                simulation_results["success_count"] += 1

        print(
            f"âœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {simulation_results['success_count']}ê°œ ì„±ê³µ, {simulation_results['error_count']}ê°œ ì¶©ëŒ"
        )
        return simulation_results

    def execute_processing(
        self,
        dry_run: bool = True,
        progress_callback: Callable[[int], None] | None = None,
        max_workers: int = 4,
    ) -> dict[str, any]:
        """íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰ - ë¦¬íŒ©í† ë§ëœ ë²„ì „"""
        if not self.processing_plans:
            return {"error": "ì²˜ë¦¬ ê³„íšì´ ì—†ìŠµë‹ˆë‹¤"}

        try:
            # Emit processing started event
            self.current_operation_id = uuid4()
            if self.event_bus:
                started_event = FileProcessingStartedEvent(
                    operation_id=self.current_operation_id,
                    operation_type="file_organization",
                    total_files=len(self.processing_plans),
                    total_size_bytes=sum(
                        plan.estimated_size * 1024 * 1024
                        for plan in self.processing_plans
                        if plan.estimated_size
                    ),
                    processing_mode="dry_run" if dry_run else "normal",
                )
                self.event_bus.publish(started_event)

            # FileProcessingPlanì„ FileOperationPlanìœ¼ë¡œ ë³€í™˜
            unified_plans = []
            for plan in self.processing_plans:
                if plan.conflicts:
                    continue  # ì¶©ëŒì´ ìˆëŠ” ê³„íšì€ ê±´ë„ˆëœ€

                operation_type = (
                    FileOperationType.COPY if plan.action == "copy" else FileOperationType.MOVE
                )

                unified_plan = FileOperationPlan(
                    source_path=Path(plan.source_path),
                    target_path=Path(plan.target_path),
                    operation_type=operation_type,
                    backup_path=Path(plan.backup_path) if plan.backup_path else None,
                    estimated_size=plan.estimated_size * 1024 * 1024,  # MBë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
                    conflict_resolution=FileConflictResolution.RENAME,
                )
                unified_plans.append(unified_plan)

            # Create detailed progress callback
            def detailed_progress_callback(progress_event: FileProcessingProgressEvent):
                if self.event_bus:
                    self.event_bus.publish(progress_event)
                # Also call simple callback for backward compatibility
                if progress_callback:
                    progress_callback(int(progress_event.progress_percentage))

            # í†µí•©ëœ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰
            if dry_run:
                print("ğŸ­ ë“œë¼ì´ ëŸ° ëª¨ë“œë¡œ íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰")
                results = self.unified_service.execute_organization_plan(
                    unified_plans,
                    dry_run=True,
                    progress_callback=progress_callback,
                    detailed_progress_callback=detailed_progress_callback,
                )
            else:
                print("ğŸš€ ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰")
                results = self.unified_service.execute_organization_plan(
                    unified_plans,
                    dry_run=False,
                    progress_callback=progress_callback,
                    detailed_progress_callback=detailed_progress_callback,
                )

            # ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            processed = []
            errors = []
            total_processed = 0
            total_errors = 0

            for result in results:
                if result.success:
                    processed.append(
                        {
                            "source": str(result.source_path),
                            "target": str(result.target_path),
                            "operation": result.operation_type.value,
                        }
                    )
                    total_processed += 1
                else:
                    errors.append(
                        {
                            "source": str(result.source_path),
                            "target": str(result.target_path),
                            "error": result.error_message or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜",
                        }
                    )
                    total_errors += 1

            # í†µê³„ ì—…ë°ì´íŠ¸
            if not dry_run:
                self.processing_stats.processed_files = total_processed
                self.processing_stats.error_files = total_errors

            print(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {total_processed}ê°œ ì„±ê³µ, {total_errors}ê°œ ì˜¤ë¥˜")

            # Emit processing completed event
            if self.event_bus:
                completed_event = FileProcessingCompletedEvent(
                    operation_id=self.current_operation_id,
                    total_files=len(unified_plans),
                    successful_files=total_processed,
                    failed_files=total_errors,
                    skipped_files=0,
                    total_size_bytes=sum(
                        plan.estimated_size for plan in self.processing_plans if plan.estimated_size
                    )
                    * 1024
                    * 1024,
                    processed_size_bytes=sum(
                        plan.estimated_size for plan in self.processing_plans if plan.estimated_size
                    )
                    * 1024
                    * 1024,
                    total_processing_time_seconds=0.0,  # Could be calculated if needed
                    errors=[error.get("error", "Unknown error") for error in errors],
                )
                self.event_bus.publish(completed_event)

            return {
                "processed": processed,
                "errors": errors,
                "total_processed": total_processed,
                "total_errors": total_errors,
            }

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            # Emit processing failed event
            if self.event_bus:
                failed_event = FileProcessingFailedEvent(
                    operation_id=self.current_operation_id,
                    error_message=str(e),
                    error_type="execution_error",
                    failed_at_step="file_processing_execution",
                    processed_files_before_failure=0,
                    total_files=len(self.processing_plans),
                    can_retry=True,
                )
                self.event_bus.publish(failed_event)

            return {"error": f"íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"}

    def _execute_single_plan(self, plan: FileProcessingPlan) -> bool:
        """ë‹¨ì¼ ì²˜ë¦¬ ê³„íš ì‹¤í–‰"""
        try:
            # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
            target_dir = Path(plan.target_path).parent
            target_dir.mkdir(parents=True, exist_ok=True)

            # ë°±ì—… ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            if plan.backup_path and Path(plan.target_path).exists():
                shutil.copy2(plan.target_path, plan.backup_path)
                print(f"ğŸ’¾ ë°±ì—… ìƒì„±: {plan.backup_path}")

            # íŒŒì¼ ì²˜ë¦¬
            if plan.action == "copy":
                shutil.copy2(plan.source_path, plan.target_path)
            elif plan.action == "move":
                shutil.move(plan.source_path, plan.target_path)

            print(
                f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {Path(plan.source_path).name} â†’ {Path(plan.target_path).name}"
            )
            return True

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {plan.source_path} - {e}")
            return False

    def get_processing_stats(self) -> ProcessingStats:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.processing_stats

    def clear_plans(self):
        """ì²˜ë¦¬ ê³„íš ì´ˆê¸°í™”"""
        self.processing_plans.clear()
        self.processing_stats = ProcessingStats()
        print("ğŸ—‘ï¸ ì²˜ë¦¬ ê³„íš ì´ˆê¸°í™” ì™„ë£Œ")

    def export_processing_report(self, filepath: str):
        """ì²˜ë¦¬ ê³„íšì„ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            report = {
                "stats": {
                    "total_files": self.processing_stats.total_files,
                    "total_size_mb": self.processing_stats.total_size_mb,
                    "estimated_time_seconds": self.processing_stats.estimated_time_seconds,
                },
                "plans": [],
            }

            for plan in self.processing_plans:
                plan_dict = {
                    "source_path": plan.source_path,
                    "target_path": plan.target_path,
                    "action": plan.action,
                    "backup_path": plan.backup_path,
                    "estimated_size": plan.estimated_size,
                    "conflicts": plan.conflicts,
                }
                report["plans"].append(plan_dict)

            import json

            with Path(filepath).open("w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            print(f"âœ… ì²˜ë¦¬ ê³„íš ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ê³„íš ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    def create_processing_plan(
        self, parsed_items: list[ParsedItem], organize_mode: str = "move"
    ) -> list[FileProcessingPlan]:
        """íŒŒì‹±ëœ ì•„ì´í…œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ê³„íš ìƒì„±"""
        if not parsed_items:
            return []

        print(f"ğŸ“‹ ì²˜ë¦¬ ê³„íš ìƒì„± ì‹œì‘: {len(parsed_items)}ê°œ ì•„ì´í…œ")

        plans = []
        for item in parsed_items:
            try:
                # ëŒ€ìƒ ê²½ë¡œ ìƒì„±
                target_path = self._generate_target_path(item)

                # ì²˜ë¦¬ ê³„íš ìƒì„±
                plan = FileProcessingPlan(
                    source_path=item.sourcePath or item.path,
                    target_path=target_path,
                    action=organize_mode,
                    estimated_size=(
                        Path(item.sourcePath or item.path).stat().st_size
                        if Path(item.sourcePath or item.path).exists()
                        else None
                    ),
                )

                # ì¶©ëŒ ê²€ì‚¬
                if Path(target_path).exists():
                    plan.conflicts.append(f"ëŒ€ìƒ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•¨: {target_path}")

                plans.append(plan)

            except Exception as e:
                print(f"âš ï¸ ì²˜ë¦¬ ê³„íš ìƒì„± ì‹¤íŒ¨ ({item.sourcePath or item.path}): {e}")

        self.processing_plans = plans
        self.processing_stats.total_files = len(plans)

        print(f"âœ… ì²˜ë¦¬ ê³„íš ìƒì„± ì™„ë£Œ: {len(plans)}ê°œ ê³„íš")
        return plans

    def simulate_organization(
        self, parsed_items: list[ParsedItem], organize_mode: str = "move"
    ) -> dict:
        """íŒŒì¼ ì •ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
        print("ğŸ­ íŒŒì¼ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")

        # ì²˜ë¦¬ ê³„íš ìƒì„±
        plans = self.create_processing_plan(parsed_items, organize_mode)

        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
        simulation_results = {
            "total_files": len(plans),
            "successful": 0,
            "conflicts": 0,
            "errors": 0,
            "details": [],
        }

        for plan in plans:
            detail = {
                "source": plan.source_path,
                "target": plan.target_path,
                "action": plan.action,
                "status": "success" if not plan.conflicts else "conflict",
                "conflicts": plan.conflicts,
            }

            if plan.conflicts:
                simulation_results["conflicts"] += 1
            else:
                simulation_results["successful"] += 1

            simulation_results["details"].append(detail)

        print(
            f"âœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {simulation_results['successful']}ê°œ ì„±ê³µ, {simulation_results['conflicts']}ê°œ ì¶©ëŒ"
        )
        return simulation_results

    def _generate_target_path(self, item: ParsedItem) -> str:
        """ëŒ€ìƒ ê²½ë¡œ ìƒì„±"""
        if not self.destination_root:
            return item.sourcePath or item.path

        # íŒŒì¼ëª… ìƒì„± (ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©)
        filename = Path(item.sourcePath or item.path).name

        # ëŒ€ìƒ ê²½ë¡œ ì¡°í•©
        return str(Path(self.destination_root) / filename)
