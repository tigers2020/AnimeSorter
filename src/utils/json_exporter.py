"""
íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ JSON ë‚´ë³´ë‚´ê¸° ìœ í‹¸ë¦¬í‹°

íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import gzip
import orjson
import io
import itertools
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Iterator
from dataclasses import dataclass, asdict, field
from enum import Enum

try:
    from src.utils.logger import get_logger
except ImportError:
    from src.utils.logger import get_logger


class ExportFormat(Enum):
    """ë‚´ë³´ë‚´ê¸° í˜•ì‹"""
    JSON = "json"
    GZIPPED_JSON = "json.gz"
    MINIFIED_JSON = "minified.json"
    STREAMING_JSON = "streaming.json"  # ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ë²„ì „


@dataclass
class ScanMetadata:
    """ìŠ¤ìº” ë©”íƒ€ë°ì´í„°"""
    scan_timestamp: str
    total_files: int
    total_groups: int
    scan_duration: float
    source_directory: str
    version: str = "1.0.0"
    export_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class FileData:
    """íŒŒì¼ ë°ì´í„°"""
    original_path: str
    file_name: str
    file_size: int
    file_extension: str
    last_modified: str
    is_video: bool
    is_subtitle: bool
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class GroupedFileData:
    """ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°"""
    title: str
    year: Optional[int]
    season: int
    episode: Optional[int]
    files: List[FileData]
    total_size: int
    file_count: int
    video_count: int
    subtitle_count: int


@dataclass
class ScanResultExport:
    """ìŠ¤ìº” ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ë°ì´í„°"""
    metadata: ScanMetadata
    groups: List[GroupedFileData]
    statistics: Dict[str, Any] = field(default_factory=dict)


class JSONExporter:
    """JSON ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def export_scan_results(self, 
                           grouped_files: Dict, 
                           source_directory: str,
                           scan_duration: float,
                           output_path: Union[str, Path],
                           format: ExportFormat = ExportFormat.JSON,
                           include_metadata: bool = True,
                           compress: bool = False) -> Path:
        """
        ìŠ¤ìº” ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (í†µí•©ëœ ë²„ì „)
        
        Args:
            grouped_files: ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°
            source_directory: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            scan_duration: ìŠ¤ìº” ì†Œìš” ì‹œê°„
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            format: ë‚´ë³´ë‚´ê¸° í˜•ì‹
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            compress: ì••ì¶• ì—¬ë¶€
            
        Returns:
            Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_path = Path(output_path)
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        if self._should_use_streaming(grouped_files, format):
            return self._export_scan_results_streaming(
                grouped_files, source_directory, scan_duration, output_path, compress
            )
        
        # ì¼ë°˜ ë°©ì‹ ì‚¬ìš©
        scan_data = self._structure_scan_data(
            grouped_files, source_directory, scan_duration
        )
        
        # JSON ì§ë ¬í™”
        json_data = self._serialize_to_json(scan_data, format)
        
        # íŒŒì¼ ì €ì¥
        return self._save_to_file(json_data, output_path, format, compress)
    
    def _should_use_streaming(self, grouped_files: Dict, format: ExportFormat) -> bool:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        
        Args:
            grouped_files: ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°
            format: ë‚´ë³´ë‚´ê¸° í˜•ì‹
            
        Returns:
            bool: ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© ì—¬ë¶€
        """
        # ëª…ì‹œì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ì´ ìš”ì²­ëœ ê²½ìš°
        if format == ExportFormat.STREAMING_JSON:
            return True
        
        # íŒŒì¼ ìˆ˜ê°€ ë§ì€ ê²½ìš° (1000ê°œ ì´ìƒ)
        total_files = sum(len(files) for files in grouped_files.values())
        if total_files > 1000:
            return True
        
        # ê·¸ë£¹ ìˆ˜ê°€ ë§ì€ ê²½ìš° (100ê°œ ì´ìƒ)
        if len(grouped_files) > 100:
            return True
        
        return False
    
    def _export_scan_results_streaming(self,
                                     grouped_files: Dict,
                                     source_directory: str,
                                     scan_duration: float,
                                     output_path: Path,
                                     compress: bool) -> Path:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìŠ¤ìº” ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (ìµœì í™”ëœ ë²„ì „)
        
        Args:
            grouped_files: ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°
            source_directory: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            scan_duration: ìŠ¤ìº” ì†Œìš” ì‹œê°„
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            compress: ì••ì¶• ì—¬ë¶€
            
        Returns:
            Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ì••ì¶• ì—¬ë¶€ì— ë”°ë¥¸ íŒŒì¼ í™•ì¥ì ê²°ì •
        if compress:
            final_path = output_path.with_suffix('.json.gz')
            file_obj = gzip.open(final_path, 'wb')
        else:
            final_path = output_path.with_suffix('.json')
            file_obj = open(final_path, 'wb')
        
        try:
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            total_files = sum(len(files) for files in grouped_files.values())
            total_groups = len(grouped_files)
            
            metadata = {
                "scan_timestamp": datetime.now().isoformat(),
                "total_files": total_files,
                "total_groups": total_groups,
                "scan_duration": scan_duration,
                "source_directory": source_directory,
                "version": "1.0.0",
                "export_timestamp": datetime.now().isoformat()
            }
            
            # ìŠ¤íŠ¸ë¦¬ë° JSON ì‘ì„± ì‹œì‘
            file_obj.write(b'{\n')
            
            # ë©”íƒ€ë°ì´í„° ì„¹ì…˜
            file_obj.write(b'  "metadata": ')
            file_obj.write(orjson.dumps(metadata, option=orjson.OPT_INDENT_2))
            file_obj.write(b',\n')
            
            # ê·¸ë£¹ ì„¹ì…˜ ì‹œì‘
            file_obj.write(b'  "groups": [\n')
            
            # ê·¸ë£¹ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
            first_group = True
            for (title, year, season), files in grouped_files.items():
                if not first_group:
                    file_obj.write(b',\n')
                first_group = False
                
                # ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ê³„ì‚°
                total_size = 0
                video_count = 0
                subtitle_count = 0
                
                # íŒŒì¼ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
                group_data = {
                    "title": title,
                    "year": year,
                    "season": season,
                    "episode": None,  # ê·¸ë£¹ ë ˆë²¨ì—ì„œëŠ” None
                    "files": []
                }
                
                for file_data in files:
                    # íŒŒì¼ ì •ë³´ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œ
                    file_info = self._extract_file_info_optimized(file_data)
                    group_data["files"].append(file_info)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    total_size += file_info.get("file_size", 0)
                    if file_info.get("is_video", False):
                        video_count += 1
                    if file_info.get("is_subtitle", False):
                        subtitle_count += 1
                
                # ê·¸ë£¹ í†µê³„ ì¶”ê°€
                group_data.update({
                    "total_size": total_size,
                    "file_count": len(files),
                    "video_count": video_count,
                    "subtitle_count": subtitle_count
                })
                
                # orjsonìœ¼ë¡œ ê·¸ë£¹ ë°ì´í„° ì§ë ¬í™” (ë“¤ì—¬ì“°ê¸° í¬í•¨)
                group_json = orjson.dumps(group_data, option=orjson.OPT_INDENT_2)
                # ì²« ë²ˆì§¸ ì¤„ì˜ ë“¤ì—¬ì“°ê¸° ì¡°ì •
                lines = group_json.split(b'\n')
                adjusted_lines = [b'    ' + line if line.strip() else line for line in lines]
                file_obj.write(b'\n'.join(adjusted_lines))
            
            # ê·¸ë£¹ ì„¹ì…˜ ì¢…ë£Œ
            file_obj.write(b'\n  ],\n')
            
            # í†µê³„ ì„¹ì…˜
            statistics = self._calculate_statistics_streaming(grouped_files)
            file_obj.write(b'  "statistics": ')
            file_obj.write(orjson.dumps(statistics, option=orjson.OPT_INDENT_2))
            file_obj.write(b'\n')
            
            # JSON ì¢…ë£Œ
            file_obj.write(b'}\n')
            
        finally:
            file_obj.close()
        
        self.logger.info(f"ìŠ¤íŠ¸ë¦¬ë° JSON ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {final_path}")
        return final_path
    
    def _extract_file_info_optimized(self, file_data: Any) -> Dict[str, Any]:
        """íŒŒì¼ ì •ë³´ë¥¼ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œ"""
        if hasattr(file_data, 'extra_info') and file_data.extra_info:
            # ì´ë¯¸ ìˆ˜ì§‘ëœ íŒŒì¼ ì •ë³´ ì‚¬ìš©
            file_info = {
                "original_path": str(getattr(file_data, "original_filename", "")),
                "file_name": Path(getattr(file_data, "original_filename", "")).name,
                "file_size": file_data.extra_info.get("file_size", 0),
                "file_extension": Path(getattr(file_data, "original_filename", "")).suffix,
                "last_modified": file_data.extra_info.get("last_modified", datetime.now().isoformat()),
                "is_video": Path(getattr(file_data, "original_filename", "")).suffix.lower() in ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.m4v', '.flv'],
                "is_subtitle": Path(getattr(file_data, "original_filename", "")).suffix.lower() in ['.srt', '.ass', '.ssa', '.sub', '.idx', '.smi', '.vtt'],
                "metadata": {
                    "title": getattr(file_data, "title", ""),
                    "season": getattr(file_data, "season", 1),
                    "episode": getattr(file_data, "episode", None),
                    "year": getattr(file_data, "year", None),
                    "is_movie": getattr(file_data, "is_movie", False)
                }
            }
        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì •ë³´ ì¡°íšŒ (ìµœì†Œí™”)
            file_path = Path(getattr(file_data, "original_filename", ""))
            try:
                stat_info = file_path.stat()
                file_size = stat_info.st_size
                last_modified = datetime.fromtimestamp(stat_info.st_mtime).isoformat()
            except (OSError, FileNotFoundError):
                file_size = 0
                last_modified = datetime.now().isoformat()
            
            file_info = {
                "original_path": str(file_path),
                "file_name": file_path.name,
                "file_size": file_size,
                "file_extension": file_path.suffix,
                "last_modified": last_modified,
                "is_video": file_path.suffix.lower() in ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.m4v', '.flv'],
                "is_subtitle": file_path.suffix.lower() in ['.srt', '.ass', '.ssa', '.sub', '.idx', '.smi', '.vtt'],
                "metadata": {
                    "title": getattr(file_data, "title", ""),
                    "season": getattr(file_data, "season", 1),
                    "episode": getattr(file_data, "episode", None),
                    "year": getattr(file_data, "year", None),
                    "is_movie": getattr(file_data, "is_movie", False)
                }
            }
        
        return file_info
    
    def _calculate_statistics_streaming(self, grouped_files: Dict) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í†µê³„ ê³„ì‚°"""
        total_size_bytes = 0
        total_video_files = 0
        total_subtitle_files = 0
        group_sizes = []
        
        for files in grouped_files.values():
            group_size = 0
            for file_data in files:
                # ìµœì í™”ëœ íŒŒì¼ í¬ê¸° ì¡°íšŒ
                if hasattr(file_data, 'extra_info') and file_data.extra_info:
                    file_size = file_data.extra_info.get("file_size", 0)
                else:
                    try:
                        file_size = Path(getattr(file_data, "original_filename", "")).stat().st_size
                    except (OSError, FileNotFoundError):
                        file_size = 0
                
                group_size += file_size
                total_size_bytes += file_size
                
                # íŒŒì¼ íƒ€ì… ì¹´ìš´íŠ¸
                file_path = Path(getattr(file_data, "original_filename", ""))
                if file_path.suffix.lower() in ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.m4v', '.flv']:
                    total_video_files += 1
                elif file_path.suffix.lower() in ['.srt', '.ass', '.ssa', '.sub', '.idx', '.smi', '.vtt']:
                    total_subtitle_files += 1
            
            group_sizes.append(group_size)
        
        return {
            "total_size_bytes": total_size_bytes,
            "total_size_gb": round(total_size_bytes / (1024**3), 2),
            "total_video_files": total_video_files,
            "total_subtitle_files": total_subtitle_files,
            "average_group_size": total_size_bytes / len(grouped_files) if grouped_files else 0,
            "largest_group": max(group_sizes) if group_sizes else 0,
            "smallest_group": min(group_sizes) if group_sizes else 0
        }
    
    def _structure_scan_data(self, 
                           grouped_files: Dict, 
                           source_directory: str,
                           scan_duration: float) -> ScanResultExport:
        """ìŠ¤ìº” ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        total_files = sum(len(files) for files in grouped_files.values())
        metadata = ScanMetadata(
            scan_timestamp=datetime.now().isoformat(),
            total_files=total_files,
            total_groups=len(grouped_files),
            scan_duration=scan_duration,
            source_directory=str(source_directory)
        )
        
        # ê·¸ë£¹ ë°ì´í„° ë³€í™˜
        groups = []
        total_size = 0
        total_video_count = 0
        total_subtitle_count = 0
        
        for (title, year, season), files in grouped_files.items():
            group_files = []
            group_size = 0
            video_count = 0
            subtitle_count = 0
            
            # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            for file_data in files:
                if hasattr(file_data, 'original_filename'):
                    file_path = Path(file_data.original_filename)
                elif isinstance(file_data, dict):
                    file_path = Path(file_data.get('original_filename', ''))
                else:
                    continue
                
                # íŒŒì¼ í¬ê¸° ì¡°íšŒ
                try:
                    file_size = file_path.stat().st_size
                except (OSError, FileNotFoundError):
                    file_size = 0
                
                # íŒŒì¼ íƒ€ì… íŒë³„
                ext = file_path.suffix.lower()
                is_video = ext in ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.m4v', '.flv']
                is_subtitle = ext in ['.srt', '.ass', '.ssa', '.sub', '.idx', '.smi', '.vtt']
                
                if is_video:
                    video_count += 1
                elif is_subtitle:
                    subtitle_count += 1
                
                # íŒŒì¼ ì •ë³´ ìƒì„±
                try:
                    last_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                except (OSError, FileNotFoundError):
                    last_modified = datetime.now().isoformat()
                
                file_info = FileData(
                    original_path=str(file_path),
                    file_name=file_path.name,
                    file_size=file_size,
                    file_extension=ext,
                    last_modified=last_modified,
                    is_video=is_video,
                    is_subtitle=is_subtitle,
                    metadata=self._extract_file_metadata(file_data)
                )
                
                group_files.append(file_info)
                group_size += file_size
            
            # ê·¸ë£¹ ì •ë³´ ìƒì„±
            group_data = GroupedFileData(
                title=title,
                year=year,
                season=season,
                episode=getattr(files[0], 'episode', None) if files else None,
                files=group_files,
                total_size=group_size,
                file_count=len(group_files),
                video_count=video_count,
                subtitle_count=subtitle_count
            )
            
            groups.append(group_data)
            total_size += group_size
            total_video_count += video_count
            total_subtitle_count += subtitle_count
        
        # í†µê³„ ì •ë³´
        statistics = {
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "total_size_gb": round(total_size / (1024 * 1024 * 1024), 2),
            "total_video_files": total_video_count,
            "total_subtitle_files": total_subtitle_count,
            "average_group_size": round(total_size / len(groups), 2) if groups else 0,
            "largest_group": max(len(g.files) for g in groups) if groups else 0,
            "smallest_group": min(len(g.files) for g in groups) if groups else 0
        }
        
        return ScanResultExport(
            metadata=metadata,
            groups=groups,
            statistics=statistics
        )
    
    def _extract_file_metadata(self, file_data: Any) -> Dict[str, Any]:
        """íŒŒì¼ ë°ì´í„°ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        if hasattr(file_data, '__dict__'):
            # ê°ì²´ì¸ ê²½ìš°
            for key, value in file_data.__dict__.items():
                if key not in ['original_filename'] and value is not None:
                    metadata[key] = value
        elif isinstance(file_data, dict):
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            for key, value in file_data.items():
                if key not in ['original_filename'] and value is not None:
                    metadata[key] = value
        
        return metadata
    
    def _serialize_to_json(self, 
                          scan_data: ScanResultExport, 
                          format: ExportFormat) -> str:
        """ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
        
        # dataclassë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        data_dict = asdict(scan_data)
        
        # JSON ì§ë ¬í™” ì˜µì…˜
        indent = None if format == ExportFormat.MINIFIED_JSON else 2
        separators = (',', ':') if format == ExportFormat.MINIFIED_JSON else None
        
        return json.dumps(
            data_dict,
            indent=indent,
            separators=separators,
            ensure_ascii=False,
            default=str
        )
    
    def _save_to_file(self, 
                     json_data: str, 
                     output_path: Path, 
                     format: ExportFormat,
                     compress: bool) -> Path:
        """JSON ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        if format == ExportFormat.GZIPPED_JSON or compress:
            with gzip.open(output_path.with_suffix('.json.gz'), 'wt', encoding='utf-8') as f:
                f.write(json_data)
            final_path = output_path.with_suffix('.json.gz')
        else:
            with open(output_path.with_suffix('.json'), 'w', encoding='utf-8') as f:
                f.write(json_data)
            final_path = output_path.with_suffix('.json')
        
        self.logger.info(f"ìŠ¤ìº” ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {final_path}")
        return final_path
    
    def load_scan_results(self, 
                         file_path: Union[str, Path]) -> ScanResultExport:
        """
        JSON íŒŒì¼ì—ì„œ ìŠ¤ìº” ê²°ê³¼ ë¡œë“œ
        
        Args:
            file_path: JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ScanResultExport: ë¡œë“œëœ ìŠ¤ìº” ê²°ê³¼
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # íŒŒì¼ ì½ê¸°
        if file_path.suffix == '.gz':
            with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                data = json.load(f)
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ dataclassë¡œ ë³€í™˜
        return self._deserialize_from_dict(data)
    
    def _deserialize_from_dict(self, data: Dict[str, Any]) -> ScanResultExport:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ScanResultExport ê°ì²´ ìƒì„±"""
        
        # ë©”íƒ€ë°ì´í„° ë³€í™˜
        metadata = ScanMetadata(**data.get('metadata', {}))
        
        # ê·¸ë£¹ ë°ì´í„° ë³€í™˜
        groups = []
        for group_data in data.get('groups', []):
            files = [FileData(**file_data) for file_data in group_data.get('files', [])]
            
            group = GroupedFileData(
                title=group_data['title'],
                year=group_data.get('year'),
                season=group_data.get('season', 1),
                episode=group_data.get('episode'),
                files=files,
                total_size=group_data.get('total_size', 0),
                file_count=group_data.get('file_count', 0),
                video_count=group_data.get('video_count', 0),
                subtitle_count=group_data.get('subtitle_count', 0)
            )
            groups.append(group)
        
        return ScanResultExport(
            metadata=metadata,
            groups=groups,
            statistics=data.get('statistics', {})
        )
    
    def get_export_summary(self, scan_data: ScanResultExport) -> str:
        """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ì •ë³´ ìƒì„±"""
        
        metadata = scan_data.metadata
        stats = scan_data.statistics
        
        summary = f"""
=== ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ===
ğŸ“ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {metadata.source_directory}
ğŸ“… ìŠ¤ìº” ì‹œê°„: {metadata.scan_timestamp}
â±ï¸  ìŠ¤ìº” ì†Œìš” ì‹œê°„: {metadata.scan_duration:.2f}ì´ˆ

ğŸ“Š íŒŒì¼ í†µê³„:
   â€¢ ì´ íŒŒì¼ ìˆ˜: {metadata.total_files:,}ê°œ
   â€¢ ê·¸ë£¹ ìˆ˜: {metadata.total_groups:,}ê°œ
   â€¢ ë¹„ë””ì˜¤ íŒŒì¼: {stats.get('total_video_files', 0):,}ê°œ
   â€¢ ìë§‰ íŒŒì¼: {stats.get('total_subtitle_files', 0):,}ê°œ

ğŸ’¾ ìš©ëŸ‰ í†µê³„:
   â€¢ ì´ ìš©ëŸ‰: {stats.get('total_size_gb', 0):.2f}GB
   â€¢ í‰ê·  ê·¸ë£¹ í¬ê¸°: {stats.get('average_group_size', 0):.2f}MB
   â€¢ ìµœëŒ€ ê·¸ë£¹: {stats.get('largest_group', 0)}ê°œ íŒŒì¼
   â€¢ ìµœì†Œ ê·¸ë£¹: {stats.get('smallest_group', 0)}ê°œ íŒŒì¼
"""
        
        return summary.strip() 


    

    
 