"""
íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ JSON ë‚´ë³´ë‚´ê¸° ìœ í‹¸ë¦¬í‹°

íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import gzip
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict, field
from enum import Enum

try:
    from utils.logger import get_logger
except ImportError:
    from src.utils.logger import get_logger


class ExportFormat(Enum):
    """ë‚´ë³´ë‚´ê¸° í˜•ì‹"""
    JSON = "json"
    GZIPPED_JSON = "json.gz"
    MINIFIED_JSON = "minified.json"


@dataclass
class ScanMetadata:
    """ìŠ¤ìº” ë©”íƒ€ë°ì´í„°"""
    scan_timestamp: str
    total_files: int
    total_groups: int
    scan_duration: float
    source_directory: str
    version: str = "1.0.0"
    export_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´"""
    original_path: str
    file_name: str
    file_size: int
    file_extension: str
    last_modified: str
    is_video: bool
    is_subtitle: bool
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class GroupedFileData:
    """ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°"""
    title: str
    year: Optional[int]
    season: int
    episode: Optional[int]
    files: List[FileInfo]
    total_size: int
    file_count: int
    video_count: int
    subtitle_count: int


@dataclass
class ScanResultExport:
    """ìŠ¤ìº” ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ë°ì´í„°"""
    metadata: ScanMetadata
    groups: List[GroupedFileData]
    statistics: Dict[str, Any] = field(default_factory=dict)


class JSONExporter:
    """JSON ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def export_scan_results(self, 
                           grouped_files: Dict, 
                           source_directory: str,
                           scan_duration: float,
                           output_path: Union[str, Path],
                           format: ExportFormat = ExportFormat.JSON,
                           include_metadata: bool = True,
                           compress: bool = False) -> Path:
        """
        ìŠ¤ìº” ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        
        Args:
            grouped_files: ê·¸ë£¹í™”ëœ íŒŒì¼ ë°ì´í„°
            source_directory: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            scan_duration: ìŠ¤ìº” ì†Œìš” ì‹œê°„
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            format: ë‚´ë³´ë‚´ê¸° í˜•ì‹
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            compress: ì••ì¶• ì—¬ë¶€
            
        Returns:
            Path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_path = Path(output_path)
        
        # ìŠ¤ìº” ê²°ê³¼ ë°ì´í„° êµ¬ì¡°í™”
        scan_data = self._structure_scan_data(
            grouped_files, source_directory, scan_duration
        )
        
        # JSON ì§ë ¬í™”
        json_data = self._serialize_to_json(scan_data, format)
        
        # íŒŒì¼ ì €ì¥
        return self._save_to_file(json_data, output_path, format, compress)
    
    def _structure_scan_data(self, 
                           grouped_files: Dict, 
                           source_directory: str,
                           scan_duration: float) -> ScanResultExport:
        """ìŠ¤ìº” ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        total_files = sum(len(files) for files in grouped_files.values())
        metadata = ScanMetadata(
            scan_timestamp=datetime.now().isoformat(),
            total_files=total_files,
            total_groups=len(grouped_files),
            scan_duration=scan_duration,
            source_directory=str(source_directory)
        )
        
        # ê·¸ë£¹ ë°ì´í„° ë³€í™˜
        groups = []
        total_size = 0
        total_video_count = 0
        total_subtitle_count = 0
        
        for (title, year, season), files in grouped_files.items():
            group_files = []
            group_size = 0
            video_count = 0
            subtitle_count = 0
            
            # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            for file_data in files:
                if hasattr(file_data, 'original_filename'):
                    file_path = Path(file_data.original_filename)
                elif isinstance(file_data, dict):
                    file_path = Path(file_data.get('original_filename', ''))
                else:
                    continue
                
                # íŒŒì¼ í¬ê¸° ì¡°íšŒ
                try:
                    file_size = file_path.stat().st_size
                except (OSError, FileNotFoundError):
                    file_size = 0
                
                # íŒŒì¼ íƒ€ì… íŒë³„
                ext = file_path.suffix.lower()
                is_video = ext in ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.m4v', '.flv']
                is_subtitle = ext in ['.srt', '.ass', '.ssa', '.sub', '.idx', '.smi', '.vtt']
                
                if is_video:
                    video_count += 1
                elif is_subtitle:
                    subtitle_count += 1
                
                # íŒŒì¼ ì •ë³´ ìƒì„±
                try:
                    last_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                except (OSError, FileNotFoundError):
                    last_modified = datetime.now().isoformat()
                
                file_info = FileInfo(
                    original_path=str(file_path),
                    file_name=file_path.name,
                    file_size=file_size,
                    file_extension=ext,
                    last_modified=last_modified,
                    is_video=is_video,
                    is_subtitle=is_subtitle,
                    metadata=self._extract_file_metadata(file_data)
                )
                
                group_files.append(file_info)
                group_size += file_size
            
            # ê·¸ë£¹ ì •ë³´ ìƒì„±
            group_data = GroupedFileData(
                title=title,
                year=year,
                season=season,
                episode=getattr(files[0], 'episode', None) if files else None,
                files=group_files,
                total_size=group_size,
                file_count=len(group_files),
                video_count=video_count,
                subtitle_count=subtitle_count
            )
            
            groups.append(group_data)
            total_size += group_size
            total_video_count += video_count
            total_subtitle_count += subtitle_count
        
        # í†µê³„ ì •ë³´
        statistics = {
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "total_size_gb": round(total_size / (1024 * 1024 * 1024), 2),
            "total_video_files": total_video_count,
            "total_subtitle_files": total_subtitle_count,
            "average_group_size": round(total_size / len(groups), 2) if groups else 0,
            "largest_group": max(len(g.files) for g in groups) if groups else 0,
            "smallest_group": min(len(g.files) for g in groups) if groups else 0
        }
        
        return ScanResultExport(
            metadata=metadata,
            groups=groups,
            statistics=statistics
        )
    
    def _extract_file_metadata(self, file_data: Any) -> Dict[str, Any]:
        """íŒŒì¼ ë°ì´í„°ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        if hasattr(file_data, '__dict__'):
            # ê°ì²´ì¸ ê²½ìš°
            for key, value in file_data.__dict__.items():
                if key not in ['original_filename'] and value is not None:
                    metadata[key] = value
        elif isinstance(file_data, dict):
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            for key, value in file_data.items():
                if key not in ['original_filename'] and value is not None:
                    metadata[key] = value
        
        return metadata
    
    def _serialize_to_json(self, 
                          scan_data: ScanResultExport, 
                          format: ExportFormat) -> str:
        """ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
        
        # dataclassë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        data_dict = asdict(scan_data)
        
        # JSON ì§ë ¬í™” ì˜µì…˜
        indent = None if format == ExportFormat.MINIFIED_JSON else 2
        separators = (',', ':') if format == ExportFormat.MINIFIED_JSON else None
        
        return json.dumps(
            data_dict,
            indent=indent,
            separators=separators,
            ensure_ascii=False,
            default=str
        )
    
    def _save_to_file(self, 
                     json_data: str, 
                     output_path: Path, 
                     format: ExportFormat,
                     compress: bool) -> Path:
        """JSON ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        if format == ExportFormat.GZIPPED_JSON or compress:
            with gzip.open(output_path.with_suffix('.json.gz'), 'wt', encoding='utf-8') as f:
                f.write(json_data)
            final_path = output_path.with_suffix('.json.gz')
        else:
            with open(output_path.with_suffix('.json'), 'w', encoding='utf-8') as f:
                f.write(json_data)
            final_path = output_path.with_suffix('.json')
        
        self.logger.info(f"ìŠ¤ìº” ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {final_path}")
        return final_path
    
    def load_scan_results(self, 
                         file_path: Union[str, Path]) -> ScanResultExport:
        """
        JSON íŒŒì¼ì—ì„œ ìŠ¤ìº” ê²°ê³¼ ë¡œë“œ
        
        Args:
            file_path: JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ScanResultExport: ë¡œë“œëœ ìŠ¤ìº” ê²°ê³¼
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # íŒŒì¼ ì½ê¸°
        if file_path.suffix == '.gz':
            with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                data = json.load(f)
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ dataclassë¡œ ë³€í™˜
        return self._deserialize_from_dict(data)
    
    def _deserialize_from_dict(self, data: Dict[str, Any]) -> ScanResultExport:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ScanResultExport ê°ì²´ ìƒì„±"""
        
        # ë©”íƒ€ë°ì´í„° ë³€í™˜
        metadata = ScanMetadata(**data.get('metadata', {}))
        
        # ê·¸ë£¹ ë°ì´í„° ë³€í™˜
        groups = []
        for group_data in data.get('groups', []):
            files = [FileInfo(**file_data) for file_data in group_data.get('files', [])]
            
            group = GroupedFileData(
                title=group_data['title'],
                year=group_data.get('year'),
                season=group_data.get('season', 1),
                episode=group_data.get('episode'),
                files=files,
                total_size=group_data.get('total_size', 0),
                file_count=group_data.get('file_count', 0),
                video_count=group_data.get('video_count', 0),
                subtitle_count=group_data.get('subtitle_count', 0)
            )
            groups.append(group)
        
        return ScanResultExport(
            metadata=metadata,
            groups=groups,
            statistics=data.get('statistics', {})
        )
    
    def get_export_summary(self, scan_data: ScanResultExport) -> str:
        """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ì •ë³´ ìƒì„±"""
        
        metadata = scan_data.metadata
        stats = scan_data.statistics
        
        summary = f"""
=== ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ===
ğŸ“ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {metadata.source_directory}
ğŸ“… ìŠ¤ìº” ì‹œê°„: {metadata.scan_timestamp}
â±ï¸  ìŠ¤ìº” ì†Œìš” ì‹œê°„: {metadata.scan_duration:.2f}ì´ˆ

ğŸ“Š íŒŒì¼ í†µê³„:
   â€¢ ì´ íŒŒì¼ ìˆ˜: {metadata.total_files:,}ê°œ
   â€¢ ê·¸ë£¹ ìˆ˜: {metadata.total_groups:,}ê°œ
   â€¢ ë¹„ë””ì˜¤ íŒŒì¼: {stats.get('total_video_files', 0):,}ê°œ
   â€¢ ìë§‰ íŒŒì¼: {stats.get('total_subtitle_files', 0):,}ê°œ

ğŸ’¾ ìš©ëŸ‰ í†µê³„:
   â€¢ ì´ ìš©ëŸ‰: {stats.get('total_size_gb', 0):.2f}GB
   â€¢ í‰ê·  ê·¸ë£¹ í¬ê¸°: {stats.get('average_group_size', 0):.2f}MB
   â€¢ ìµœëŒ€ ê·¸ë£¹: {stats.get('largest_group', 0)}ê°œ íŒŒì¼
   â€¢ ìµœì†Œ ê·¸ë£¹: {stats.get('smallest_group', 0)}ê°œ íŒŒì¼
"""
        
        return summary.strip() 